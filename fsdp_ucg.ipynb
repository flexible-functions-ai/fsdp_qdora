{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d58fcb-2616-43c4-b347-8705a26187e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Installing Python packages...\n",
      "✅ Installed torch\n",
      "✅ Installed torchvision\n",
      "✅ Installed torchaudio\n",
      "✅ Installed transformers\n",
      "✅ Installed datasets\n",
      "✅ Installed accelerate\n",
      "✅ Installed peft\n",
      "✅ Installed bitsandbytes>=0.43.0\n",
      "✅ Installed safetensors\n",
      "✅ Installed fastcore\n",
      "✅ Installed requests\n",
      "\n",
      "📥 Downloading repository...\n",
      "❌ urllib failed: urlretrieve() got an unexpected keyword argument 'context'\n",
      "🔄 Trying with requests...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'github.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'codeload.github.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded using requests\n",
      "📂 Extracting repository...\n",
      "✅ Setup complete! Repository is in ./fsdp_qlora\n",
      "🚀 train.py found - ready to train!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import ssl\n",
    "\n",
    "# Create an SSL context that doesn't verify certificates\n",
    "ssl_context = ssl.create_default_context()\n",
    "ssl_context.check_hostname = False\n",
    "ssl_context.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Install packages first\n",
    "print(\"🔧 Installing Python packages...\")\n",
    "packages = [\n",
    "    \"torch\", \"torchvision\", \"torchaudio\", \n",
    "    \"transformers\", \"datasets\", \"accelerate\", \"peft\",\n",
    "    \"bitsandbytes>=0.43.0\", \"safetensors\", \"fastcore\", \"requests\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", package], \n",
    "                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        print(f\"✅ Installed {package}\")\n",
    "    except:\n",
    "        print(f\"❌ Failed to install {package}\")\n",
    "\n",
    "# Download with SSL verification disabled\n",
    "print(\"\\n📥 Downloading repository...\")\n",
    "url = \"https://github.com/AnswerDotAI/fsdp_qlora/archive/refs/heads/main.zip\"\n",
    "\n",
    "try:\n",
    "    # Use urllib with disabled SSL verification\n",
    "    urllib.request.urlretrieve(url, \"fsdp_qlora.zip\", context=ssl_context)\n",
    "    print(\"✅ Downloaded using urllib\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ urllib failed: {e}\")\n",
    "    \n",
    "    # Fallback to requests\n",
    "    try:\n",
    "        import requests\n",
    "        print(\"🔄 Trying with requests...\")\n",
    "        response = requests.get(url, verify=False)\n",
    "        with open(\"fsdp_qlora.zip\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"✅ Downloaded using requests\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ requests also failed: {e2}\")\n",
    "\n",
    "# Extract if download succeeded\n",
    "if os.path.exists(\"fsdp_qlora.zip\"):\n",
    "    print(\"📂 Extracting repository...\")\n",
    "    \n",
    "    # Clean up existing directory\n",
    "    if os.path.exists(\"fsdp_qlora\"):\n",
    "        import shutil\n",
    "        shutil.rmtree(\"fsdp_qlora\")\n",
    "    \n",
    "    with zipfile.ZipFile(\"fsdp_qlora.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    \n",
    "    os.rename(\"fsdp_qlora-main\", \"fsdp_qlora\")\n",
    "    os.remove(\"fsdp_qlora.zip\")\n",
    "    print(\"✅ Setup complete! Repository is in ./fsdp_qlora\")\n",
    "    \n",
    "    # Verify\n",
    "    if os.path.exists(\"fsdp_qlora/train.py\"):\n",
    "        print(\"🚀 train.py found - ready to train!\")\n",
    "    else:\n",
    "        print(\"❌ train.py not found\")\n",
    "else:\n",
    "    print(\"❌ Download failed completely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83592ebe-1db4-4b0e-a4da-5c87b020abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /root/.local/lib/python3.11/site-packages (0.33.4)\n",
      "Requirement already satisfied: filelock in /root/.local/lib/python3.11/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/.local/lib/python3.11/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/.local/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /root/.local/lib/python3.11/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf5fa6a-b60f-4767-85a4-a9e22cc1d203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/test/cu118\n",
      "Collecting llama-recipes\n",
      "  Downloading llama_recipes-0.0.5.post2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: fastcore in /root/.local/lib/python3.11/site-packages (1.8.5)\n",
      "Requirement already satisfied: transformers!=4.38.*,!=4.39.* in /root/.local/lib/python3.11/site-packages (4.53.2)\n",
      "Collecting llama-cookbook==0.0.5.post1 (from llama-recipes)\n",
      "  Downloading llama_cookbook-0.0.5.post1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: accelerate in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1->llama-recipes) (1.9.0)\n",
      "Collecting appdirs (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: bitsandbytes in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1->llama-recipes) (0.46.1)\n",
      "Collecting black (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
      "Collecting chardet (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting codeshield (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading codeshield-1.0.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: datasets in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1->llama-recipes) (4.0.0)\n",
      "Collecting evaluate (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting fire (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading gradio-5.38.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting loralib (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting markupsafe==2.0.1 (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading MarkupSafe-2.0.1.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting matplotlib (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting openai (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading openai-1.97.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting optimum (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading optimum-1.26.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: peft in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1->llama-recipes) (0.16.0)\n",
      "Collecting py7zr (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pyyaml==6.0.1 (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading https://download.pytorch.org/whl/test/PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge-score (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scipy (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting sentence-transformers (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting sentencepiece (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading https://download.pytorch.org/whl/test/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tabulate (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: torch>=2.2 in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1->llama-recipes) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1->llama-recipes) (4.12.2)\n",
      "Collecting unstructured[pdf] (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading unstructured-0.18.9-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from fastcore) (25.0)\n",
      "Requirement already satisfied: filelock in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (2.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers!=4.38.*,!=4.39.*) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /root/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers!=4.38.*,!=4.39.*) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /root/.local/lib/python3.11/site-packages (from torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.3.1->torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (68.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/.local/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.2->llama-cookbook==0.0.5.post1->llama-recipes) (1.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from accelerate->llama-cookbook==0.0.5.post1->llama-recipes) (7.0.0)\n",
      "Collecting click>=8.0.0 (from black->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mypy-extensions>=0.4.3 (from black->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/site-packages (from black->llama-cookbook==0.0.5.post1->llama-recipes) (4.3.8)\n",
      "Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.11/site-packages (from black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (9.4.0)\n",
      "Collecting tokenize-rt>=3.2.0 (from black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading tokenize_rt-6.2.0-py2.py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /usr/local/lib/python3.11/site-packages (from ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (0.7.0)\n",
      "Collecting semgrep>1.68 (from codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading semgrep-1.128.1-cp39.cp310.cp311.py39.py310.py311-none-musllinux_1_0_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: attrs>=21.3 in /usr/local/lib/python3.11/site-packages (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes) (24.2.0)\n",
      "Collecting boltons~=21.0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading boltons-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting click-option-group~=0.5 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading click_option_group-0.5.7-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting click>=8.0.0 (from black->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting colorama~=0.4.0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading https://download.pytorch.org/whl/test/colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: defusedxml~=0.7.1 in /usr/local/lib/python3.11/site-packages (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes) (0.7.1)\n",
      "Collecting exceptiongroup~=1.2.0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting glom~=22.1 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading glom-22.1.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: jsonschema~=4.6 in /usr/local/lib/python3.11/site-packages (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes) (4.24.0)\n",
      "Collecting opentelemetry-api~=1.25.0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-sdk~=1.25.0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http~=1.25.0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.56b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting peewee~=3.14 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading peewee-3.18.2.tar.gz (949 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.2/949.2 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rich~=13.5.2 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading rich-13.5.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ruamel.yaml>=0.18.5 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting tomli~=2.0.1 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading tomli-2.0.2-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/site-packages (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes) (2.5.0)\n",
      "Collecting wcmatch~=8.3 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading wcmatch-8.5.2-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting face>=20.1.0 (from glom~=22.1->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading face-24.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/site-packages (from jsonschema~=4.6->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/site-packages (from jsonschema~=4.6->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/site-packages (from jsonschema~=4.6->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes) (0.26.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api~=1.25.0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<=7.1,>=6.0 (from opentelemetry-api~=1.25.0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading https://download.pytorch.org/whl/test/importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<=7.1,>=6.0->opentelemetry-api~=1.25.0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-http~=1.25.0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-http~=1.25.0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-http~=1.25.0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf<5.0,>=3.19 (from opentelemetry-proto==1.25.0->opentelemetry-exporter-otlp-proto-http~=1.25.0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-instrumentation==0.56b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.56b0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.56b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.56b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.56b0->opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "INFO: pip is looking at multiple versions of opentelemetry-semantic-conventions to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.55b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation==0.55b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.55b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.55b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.55b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation==0.55b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.55b0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.55b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.55b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.55b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.55b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.54b1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-instrumentation==0.54b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.54b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.54b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.54b0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-instrumentation==0.54b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.54b0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.54b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.54b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.53b1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.53b0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-instrumentation==0.53b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.53b0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.53b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.53b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.53b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.52b1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)\n",
      "INFO: pip is still looking at multiple versions of opentelemetry-semantic-conventions to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.52b0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-instrumentation==0.52b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.52b0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.52b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.52b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.52b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.52b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.51b0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.49b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.49b1 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.49b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.49b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.49b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation_requests-0.46b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->transformers!=4.38.*,!=4.39.*) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->transformers!=4.38.*,!=4.39.*) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->transformers!=4.38.*,!=4.39.*) (2024.8.30)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich~=13.5.2->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting bracex>=2.1.1 (from wcmatch~=8.3->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading bracex-2.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich~=13.5.2->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.18.5->semgrep>1.68->codeshield->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/.local/lib/python3.11/site-packages (from datasets->llama-cookbook==0.0.5.post1->llama-recipes) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/.local/lib/python3.11/site-packages (from datasets->llama-cookbook==0.0.5.post1->llama-recipes) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/.local/lib/python3.11/site-packages (from datasets->llama-cookbook==0.0.5.post1->llama-recipes) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /root/.local/lib/python3.11/site-packages (from datasets->llama-cookbook==0.0.5.post1->llama-recipes) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/.local/lib/python3.11/site-packages (from datasets->llama-cookbook==0.0.5.post1->llama-recipes) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (3.10.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (1.13.1)\n",
      "Collecting termcolor (from fire->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/site-packages (from gradio->llama-cookbook==0.0.5.post1->llama-recipes) (4.9.0)\n",
      "Collecting brotli>=1.1.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting ffmpy (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.11.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading gradio_client-1.11.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/site-packages (from gradio->llama-cookbook==0.0.5.post1->llama-recipes) (0.28.1)\n",
      "Collecting orjson~=3.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading orjson-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /root/.local/lib/python3.11/site-packages (from gradio->llama-cookbook==0.0.5.post1->llama-recipes) (11.3.0)\n",
      "Collecting pydantic<2.12,>=2.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting pydub (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading ruff-0.12.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading starlette-0.47.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.11.0->gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio->llama-cookbook==0.0.5.post1->llama-recipes) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio->llama-cookbook==0.0.5.post1->llama-recipes) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio->llama-cookbook==0.0.5.post1->llama-recipes) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/.local/lib/python3.11/site-packages (from pandas->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/.local/lib/python3.11/site-packages (from pandas->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.12,>=2.0->gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0->gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<2.12,>=2.0->gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->llama-cookbook==0.0.5.post1->llama-recipes) (1.17.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading https://download.pytorch.org/whl/test/cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting texttable (from py7zr->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pycryptodomex>=3.20.0 (from py7zr->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting pyzstd>=0.16.1 (from py7zr->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting pyppmd<1.3.0,>=1.1.0 (from py7zr->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting absl-py (from rouge-score->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting nltk (from rouge-score->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting joblib (from nltk->rouge-score->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting scikit-learn (from sentence-transformers->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/site-packages (from stack_data->ipython>=7.8.0->black[jupyter]->llama-cookbook==0.0.5.post1->llama-recipes) (0.2.3)\n",
      "Collecting filetype (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/site-packages (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes) (4.13.4)\n",
      "Collecting emoji (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dataclasses-json (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting python-iso639 (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langdetect (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m190.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rapidfuzz (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting unstructured-client (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading unstructured_client-0.39.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting python-oxmsg (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting onnx>=1.17.0 (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting onnxruntime>=1.19.0 (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting pdf2image (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pdfminer.six (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pikepdf (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pikepdf-9.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting pi-heif (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pypdf (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pypdf-5.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting google-cloud-vision (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting effdet (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting unstructured-inference>=1.0.5 (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading unstructured_inference-1.0.5-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.19.0->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.19.0->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting opencv-python!=4.7.0.68 (from unstructured-inference>=1.0.5->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting timm (from unstructured-inference>=1.0.5->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading timm-1.0.17-py3-none-any.whl.metadata (59 kB)\n",
      "Collecting pypdfium2 (from unstructured-inference>=1.0.5->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Collecting numpy>=1.17 (from transformers!=4.38.*,!=4.39.*)\n",
      "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes) (2.7)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19.0->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading https://download.pytorch.org/whl/test/typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: torchvision in /root/.local/lib/python3.11/site-packages (from effdet->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes) (0.22.1)\n",
      "Collecting pycocotools>=2.0.2 (from effdet->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pycocotools-2.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading https://download.pytorch.org/whl/test/omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading https://download.pytorch.org/whl/test/antlr4_python3_runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/site-packages (from html5lib->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes) (0.5.1)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading cryptography-45.0.5-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in /usr/local/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes) (2.22)\n",
      "Collecting olefile (from python-oxmsg->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/site-packages (from unstructured-client->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes) (1.6.0)\n",
      "Collecting requests-toolbelt>=1.0.0 (from unstructured-client->unstructured[pdf]->llama-cookbook==0.0.5.post1->llama-recipes)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Downloading llama_recipes-0.0.5.post2-py3-none-any.whl (20 kB)\n",
      "Downloading llama_cookbook-0.0.5.post1-py3-none-any.whl (70 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading tokenize_rt-6.2.0-py2.py3-none-any.whl (6.0 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading codeshield-1.0.1-py3-none-any.whl (173 kB)\n",
      "Downloading semgrep-1.128.1-cp39.cp310.cp311.py39.py310.py311-none-musllinux_1_0_x86_64.manylinux2014_x86_64.whl (48.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading boltons-21.0.0-py2.py3-none-any.whl (193 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading click_option_group-0.5.7-py3-none-any.whl (11 kB)\n",
      "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading glom-22.1.0-py2.py3-none-any.whl (100 kB)\n",
      "Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading opentelemetry_instrumentation_requests-0.46b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
      "Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
      "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading rich-13.5.3-py3-none-any.whl (239 kB)\n",
      "Downloading tomli-2.0.2-py3-none-any.whl (13 kB)\n",
      "Downloading wcmatch-8.5.2-py3-none-any.whl (39 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Downloading bracex-2.6-py3-none-any.whl (11 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading face-24.0.0-py3-none-any.whl (54 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Downloading gradio-5.38.0-py3-none-any.whl (59.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m139.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.11.0-py3-none-any.whl (324 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (127 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m160.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m157.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.12.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m140.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
      "Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
      "Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m170.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading openai-1.97.0-py3-none-any.whl (764 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m765.0/765.0 kB\u001b[0m \u001b[31m190.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading optimum-1.26.1-py3-none-any.whl (424 kB)\n",
      "Downloading py7zr-1.0.0-py3-none-any.whl (69 kB)\n",
      "Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)\n",
      "Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
      "Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m173.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m168.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m149.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading unstructured-0.18.9-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m163.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m151.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured_inference-1.0.5-py3-none-any.whl (48 kB)\n",
      "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m154.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading https://download.pytorch.org/whl/test/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "Downloading pycocotools-2.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (477 kB)\n",
      "Downloading timm-1.0.17-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m163.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m191.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m184.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m144.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m149.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m150.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-45.0.5-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m165.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m197.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pikepdf-9.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m144.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-5.8.0-py3-none-any.whl (309 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m140.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m154.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured_client-0.39.1-py3-none-any.whl (212 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Building wheels for collected packages: markupsafe, peewee, fire, rouge-score, antlr4-python3-runtime, langdetect\n",
      "\u001b[33m  DEPRECATION: Building 'markupsafe' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'markupsafe'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for markupsafe (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for markupsafe: filename=MarkupSafe-2.0.1-py3-none-any.whl size=9745 sha256=7e2d66f9e7f03fb5c9650b1bb42b497988d9caf286498a83e97842c9bd37bfd8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/ea/18/79/6266ea508b8164a77b95aa19534c77eb805f2878612c37efca\n",
      "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peewee: filename=peewee-3.18.2-py3-none-any.whl size=139106 sha256=74775c98fa5491eac6deed3b5a8d8c2e44da24939d4780973eaec27fdde604ca\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/28/84/61/758d1bd7b9c9d700158c8642a8aff2a9bf2e1ae69641c40784\n",
      "\u001b[33m  DEPRECATION: Building 'fire' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'fire'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=604933afdfa2c129d2a0233ffaccf9ce24822ded3a417ab33a6f781ddc81d7af\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "\u001b[33m  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=0831f9eb0a69648a284b2f8bc386bdee2e9c262f67eb5d95bd6bffb09eaec1fc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "\u001b[33m  DEPRECATION: Building 'antlr4-python3-runtime' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'antlr4-python3-runtime'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=187021b4cd7030f0ba2c29c20fdd1655628f2e6f082d8f5ae91f16dd343995bd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/56/e9/6d/b5ab1c9ab438ad8897f796286bf23cd4ffc0f1ea8bc2200ecd\n",
      "\u001b[33m  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=a906ce17a25a949ae03647d998c1541f9c12b603f2c439fdef6983b2076421fc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built markupsafe peewee fire rouge-score antlr4-python3-runtime langdetect\n",
      "Installing collected packages: texttable, sentencepiece, pydub, peewee, flatbuffers, filetype, brotli, boltons, appdirs, antlr4-python3-runtime, zipp, wrapt, websockets, unstructured.pytesseract, typing-extensions, tomlkit, tomli, tokenize-rt, threadpoolctl, termcolor, tabulate, shellingham, semantic-version, ruff, ruamel.yaml.clib, rapidfuzz, pyyaml, python-multipart, python-magic, python-iso639, pyppmd, pypdfium2, pypdf, pyparsing, pycryptodomex, pybcj, pyasn1, protobuf, pi-heif, pdf2image, pathspec, orjson, opentelemetry-util-http, olefile, numpy, mypy-extensions, multivolumefile, mdurl, marshmallow, markupsafe, lxml, loralib, langdetect, kiwisolver, joblib, jiter, inflate64, humanfriendly, html5lib, grpcio, groovy, fonttools, ffmpy, face, exceptiongroup, emoji, distro, cycler, colorama, click, chardet, cachetools, bracex, backoff, annotated-types, aiofiles, absl-py, wcmatch, uvicorn, typing-inspection, typing-inspect, scipy, ruamel.yaml, rsa, requests-toolbelt, pyzstd, python-oxmsg, pydantic-core, pycocotools, pyasn1-modules, proto-plus, opentelemetry-proto, opencv-python, onnx, omegaconf, nltk, markdown-it-py, importlib-metadata, googleapis-common-protos, glom, fire, deprecated, cryptography, contourpy, coloredlogs, click-option-group, black, starlette, scikit-learn, rouge-score, rich, pydantic, py7zr, pikepdf, pdfminer.six, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, matplotlib, grpcio-status, google-auth, dataclasses-json, unstructured-client, typer, safehttpx, opentelemetry-semantic-conventions, opentelemetry-instrumentation, openai, gradio-client, google-api-core, fastapi, unstructured, timm, sentence-transformers, optimum, opentelemetry-sdk, opentelemetry-instrumentation-requests, gradio, evaluate, unstructured-inference, opentelemetry-exporter-otlp-proto-http, google-cloud-vision, effdet, semgrep, codeshield, llama-cookbook, llama-recipes\n",
      "\u001b[2K  Attempting uninstall: typing-extensions━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 12/147\u001b[0m [websockets]on3-runtime]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.12.2━━━━━\u001b[0m \u001b[32m 12/147\u001b[0m [websockets]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.12.2:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 12/147\u001b[0m [websockets]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.12.2━━━━━━━━━━━\u001b[0m \u001b[32m 14/147\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: pyyaml90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 25/147\u001b[0m [rapidfuzz]ons]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 25/147\u001b[0m [rapidfuzz]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 25/147\u001b[0m [rapidfuzz]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 25/147\u001b[0m [rapidfuzz]\n",
      "\u001b[2K  Attempting uninstall: protobuf[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 36/147\u001b[0m [pyasn1]odomex]\n",
      "\u001b[2K    Found existing installation: protobuf 5.29.5━━━━━━━━━━━━━━\u001b[0m \u001b[32m 36/147\u001b[0m [pyasn1]\n",
      "\u001b[2K    Uninstalling protobuf-5.29.5:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 36/147\u001b[0m [pyasn1]\n",
      "\u001b[2K      Successfully uninstalled protobuf-5.29.5━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 37/147\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: numpy[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 37/147\u001b[0m [protobuf]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.1━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 44/147\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling numpy-2.3.1:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 44/147\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 44/147\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: markupsafe[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 44/147\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━\u001b[0m \u001b[32m 44/147\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling MarkupSafe-3.0.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 44/147\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 49/147\u001b[0m [markupsafe]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147/147\u001b[0m [llama-recipes]hield]mgrep]loud-vision]ce]ons]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 aiofiles-24.1.0 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 backoff-2.2.1 black-25.1.0 boltons-21.0.0 bracex-2.6 brotli-1.1.0 cachetools-5.5.2 chardet-5.2.0 click-8.1.8 click-option-group-0.5.7 codeshield-1.0.1 colorama-0.4.6 coloredlogs-15.0.1 contourpy-1.3.2 cryptography-45.0.5 cycler-0.12.1 dataclasses-json-0.6.7 deprecated-1.2.18 distro-1.9.0 effdet-0.4.1 emoji-2.14.1 evaluate-0.4.5 exceptiongroup-1.2.2 face-24.0.0 fastapi-0.116.1 ffmpy-0.6.0 filetype-1.2.0 fire-0.7.0 flatbuffers-25.2.10 fonttools-4.59.0 glom-22.1.0 google-api-core-2.25.1 google-auth-2.40.3 google-cloud-vision-3.10.2 googleapis-common-protos-1.70.0 gradio-5.38.0 gradio-client-1.11.0 groovy-0.1.2 grpcio-1.73.1 grpcio-status-1.62.3 html5lib-1.1 humanfriendly-10.0 importlib-metadata-7.1.0 inflate64-1.0.3 jiter-0.10.0 joblib-1.5.1 kiwisolver-1.4.8 langdetect-1.0.9 llama-cookbook-0.0.5.post1 llama-recipes-0.0.5.post2 loralib-0.1.2 lxml-6.0.0 markdown-it-py-3.0.0 markupsafe-2.0.1 marshmallow-3.26.1 matplotlib-3.10.3 mdurl-0.1.2 multivolumefile-0.2.3 mypy-extensions-1.1.0 nltk-3.9.1 numpy-2.2.6 olefile-0.47 omegaconf-2.3.0 onnx-1.18.0 onnxruntime-1.22.1 openai-1.97.0 opencv-python-4.12.0.88 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-http-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-requests-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 optimum-1.26.1 orjson-3.11.0 pathspec-0.12.1 pdf2image-1.17.0 pdfminer.six-20250506 peewee-3.18.2 pi-heif-1.0.0 pikepdf-9.10.2 proto-plus-1.26.1 protobuf-4.25.8 py7zr-1.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybcj-1.0.6 pycocotools-2.0.10 pycryptodomex-3.23.0 pydantic-2.11.7 pydantic-core-2.33.2 pydub-0.25.1 pyparsing-3.2.3 pypdf-5.8.0 pypdfium2-4.30.1 pyppmd-1.2.0 python-iso639-2025.2.18 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.2 pyyaml-6.0.1 pyzstd-0.17.0 rapidfuzz-3.13.0 requests-toolbelt-1.0.0 rich-13.5.3 rouge-score-0.1.2 rsa-4.9.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 ruff-0.12.4 safehttpx-0.1.6 scikit-learn-1.7.1 scipy-1.16.0 semantic-version-2.10.0 semgrep-1.128.1 sentence-transformers-5.0.0 sentencepiece-0.2.0 shellingham-1.5.4 starlette-0.47.1 tabulate-0.9.0 termcolor-3.1.0 texttable-1.7.0 threadpoolctl-3.6.0 timm-1.0.17 tokenize-rt-6.2.0 tomli-2.0.2 tomlkit-0.13.3 typer-0.16.0 typing-extensions-4.14.1 typing-inspect-0.9.0 typing-inspection-0.4.1 unstructured-0.18.9 unstructured-client-0.39.1 unstructured-inference-1.0.5 unstructured.pytesseract-0.3.15 uvicorn-0.35.0 wcmatch-8.5.2 websockets-15.0.1 wrapt-1.17.2 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-recipes fastcore \"transformers!=4.38.*,!=4.39.*\" --extra-index-url https://download.pytorch.org/whl/test/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a02b12-294b-4f3d-b566-39eb9d29aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bitsandbytes>=0.43.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439a6ec6-fb84-4427-a04b-bfe3c67f006e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Hugging Face token:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import getpass\n",
    "\n",
    "# Get your token securely\n",
    "hf_token = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "\n",
    "# Login programmatically\n",
    "login(token=hf_token)\n",
    "\n",
    "print(\"✅ Successfully logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a83d80ab-9c93-4157-b941-c3011a38b4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: pip install transformers==4.47.1 --upgrade\n",
      "✅ Success\n",
      "Running: pip install bitsandbytes>=0.43.0 --upgrade --force-reinstall\n",
      "✅ Success\n",
      "✅ Essential dependencies updated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set environment variable\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# Install ONLY the essential fixes\n",
    "commands = [\n",
    "    [\"pip\", \"install\", \"transformers==4.47.1\", \"--upgrade\"],\n",
    "    [\"pip\", \"install\", \"bitsandbytes>=0.43.0\", \"--upgrade\", \"--force-reinstall\"]\n",
    "]\n",
    "\n",
    "for cmd in commands:\n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "        else:\n",
    "            print(\"✅ Success\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"⚠️ Command timed out\")\n",
    "\n",
    "print(\"✅ Essential dependencies updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33fe477-2834-4f5f-887a-515590f3733c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Applying transformers fix...\n",
      "✅ train.py fixed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"fsdp_qlora\")\n",
    "\n",
    "# Apply fixes to train.py\n",
    "with open(\"train.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Apply transformers fix\n",
    "if \"LLAMA_ATTENTION_CLASSES\" in content:\n",
    "    print(\"🔧 Applying transformers fix...\")\n",
    "    \n",
    "    # Simple replacement approach\n",
    "    content = content.replace(\n",
    "        \"LLAMA_ATTENTION_CLASSES,\", \n",
    "        \"LlamaAttention,\"\n",
    "    )\n",
    "    content = content.replace(\n",
    "        \"MISTRAL_ATTENTION_CLASSES,\", \n",
    "        \"MistralAttention,\"\n",
    "    )\n",
    "    content = content.replace(\n",
    "        \"(*LLAMA_ATTENTION_CLASSES.values(), *MISTRAL_ATTENTION_CLASSES.values())\",\n",
    "        \"(LlamaAttention, MistralAttention)\"\n",
    "    )\n",
    "    \n",
    "    # Add dataset choice\n",
    "    if \"uganda_clinical_guidelines\" not in content:\n",
    "        content = content.replace(\n",
    "            '\"orca_math\"]) = \"alpaca_sample\",',\n",
    "            '\"orca_math\", \"uganda_clinical_guidelines\"]) = \"alpaca_sample\",'\n",
    "        )\n",
    "    \n",
    "    with open(\"train.py\", \"w\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    print(\"✅ train.py fixed\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28272ac-9a03-451e-9648-81d9105f3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Bitsandbytes issue: Failed to find C compiler. Please specify via CC environment variable.\n",
      "Ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Test if fixes work\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(\"✅ Bitsandbytes works\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Bitsandbytes issue: {e}\")\n",
    "\n",
    "print(\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d451b7c0-f06e-4749-ad8a-1b5a33de860a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Setting up environment for training...\n",
      "📦 Installing build tools...\n",
      "✅ Build tools installed\n",
      "✅ Bitsandbytes imports successfully\n",
      "🚀 Environment ready! Running training...\n",
      "🧪 Running test training...\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "World size: 2\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "\n",
      "Generating train split:   0%|          | 0/130 [00:00<?, ? examples/s]\n",
      "Generating train split: 100%|██████████| 130/130 [00:00<00:00, 23549.26 examples/s]\n",
      "Creating model 0\n",
      "\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:48<00:48, 48.18s/it]\n",
      "Downloading shards:  50%|█████     | 1/2 [00:48<00:48, 48.21s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [01:04<00:00, 29.56s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [01:04<00:00, 32.35s/it]\n",
      "Loading model 0\n",
      "\n",
      "Loading & Quantizing Model Shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [01:04<00:00, 29.57s/it]\n",
      "Downloading shards: 100%|██████████| 2/2 [01:04<00:00, 32.36s/it]\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "\n",
      "Loading & Quantizing Model Shards:  50%|█████     | 1/2 [00:15<00:15, 15.38s/it]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "\n",
      "Loading & Quantizing Model Shards: 100%|██████████| 2/2 [00:25<00:00, 12.20s/it]\n",
      "Loading & Quantizing Model Shards: 100%|██████████| 2/2 [00:25<00:00, 12.68s/it]\n",
      "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.\n",
      "  warnings.warn(  # warn only once\n",
      "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.\n",
      "  warnings.warn(  # warn only once\n",
      "Rank 0: Model created: 0.107 GiB\n",
      "Using BNB DORA 0\n",
      "Rank 0: LoRA layers added: 0.107 GiB\n",
      "Wrapping model w/ FSDP 0\n",
      "Rank 0: Wrapped model: 1.625 GiB\n",
      "Applying activation checkpointing 0\n",
      "Total Training Steps: 5\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Epoch 0, Loss 0.000:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Epoch 0, Loss 0.000:  20%|██        | 1/5 [00:07<00:28,  7.01s/it]\n",
      "Epoch 0, Loss 1.388, LR 1.00e-05:  20%|██        | 1/5 [00:07<00:28,  7.01s/it]\n",
      "Epoch 0, Loss 1.388, LR 1.00e-05:  40%|████      | 2/5 [00:09<00:12,  4.30s/it]\n",
      "Epoch 0, Loss 1.477, LR 1.00e-05:  40%|████      | 2/5 [00:09<00:12,  4.30s/it]\n",
      "Epoch 0, Loss 1.477, LR 1.00e-05:  60%|██████    | 3/5 [00:11<00:06,  3.23s/it]\n",
      "Epoch 0, Loss 1.187, LR 1.00e-05:  60%|██████    | 3/5 [00:11<00:06,  3.23s/it]\n",
      "Epoch 0, Loss 1.187, LR 1.00e-05:  80%|████████  | 4/5 [00:13<00:02,  2.71s/it]\n",
      "Epoch 0, Loss 1.041, LR 1.00e-05:  80%|████████  | 4/5 [00:13<00:02,  2.71s/it]\n",
      "Epoch 0, Loss 1.041, LR 1.00e-05: 100%|██████████| 5/5 [00:15<00:00,  2.43s/it]\n",
      "Epoch 0, Loss 1.475, LR 1.00e-05: 100%|██████████| 5/5 [00:15<00:00,  2.43s/it]\n",
      "Epoch 0, Loss 1.475, LR 1.00e-05: 100%|██████████| 5/5 [00:15<00:00,  3.12s/it]\n",
      "Finished training 0\n",
      "CUDA event elapsed time: 15.2380859375 sec\n",
      "time_taken: 15.2380859375\n",
      "Rank 0: Before forward: 1.62 GiB\n",
      "Rank 0: After forward: 2.46 GiB\n",
      "Rank 0: After backward: 2.64 GiB\n",
      "Rank 0: Peak allocated memory: 1.35 GiB\n",
      "Rank 0: Peak reserved memory:  2.65 GiB\n",
      "Using BNB DORA 1\n",
      "Test completed: 0\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Setup the environment to avoid compiler issues\"\"\"\n",
    "    \n",
    "    print(\"🔧 Setting up environment for training...\")\n",
    "    \n",
    "    # Step 1: Set environment variables\n",
    "    os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "    os.environ['CC'] = '/usr/bin/gcc'\n",
    "    os.environ['CXX'] = '/usr/bin/g++'\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "    \n",
    "    # Step 2: Install build tools if possible\n",
    "    try:\n",
    "        print(\"📦 Installing build tools...\")\n",
    "        subprocess.run([\"apt\", \"update\"], capture_output=True, timeout=60)\n",
    "        result = subprocess.run([\"apt\", \"install\", \"-y\", \"build-essential\", \"gcc\", \"g++\"], \n",
    "                              capture_output=True, timeout=120)\n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ Build tools installed\")\n",
    "        else:\n",
    "            print(\"⚠️ Build tools installation failed, proceeding anyway...\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not install build tools: {e}\")\n",
    "    \n",
    "    # Step 3: Test if bitsandbytes works now\n",
    "    try:\n",
    "        import bitsandbytes\n",
    "        print(\"✅ Bitsandbytes imports successfully\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Bitsandbytes still has issues: {e}\")\n",
    "        \n",
    "        # Step 4: Try installing older version\n",
    "        print(\"🔄 Trying older bitsandbytes version...\")\n",
    "        try:\n",
    "            subprocess.run([\"pip\", \"uninstall\", \"bitsandbytes\", \"-y\"], capture_output=True)\n",
    "            subprocess.run([\"pip\", \"install\", \"bitsandbytes==0.41.3\"], capture_output=True)\n",
    "            \n",
    "            import bitsandbytes\n",
    "            print(\"✅ Older bitsandbytes version works\")\n",
    "            return True\n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Even older version failed: {e2}\")\n",
    "            return False\n",
    "\n",
    "# Run the setup\n",
    "if setup_environment():\n",
    "    print(\"🚀 Environment ready! Running training...\")\n",
    "    \n",
    "    # Your training command\n",
    "    cmd = [\n",
    "        \"python\", \"train.py\",\n",
    "        \"--train_type\", \"bnb_dora\",\n",
    "        \"--model_name\", \"meta-llama/Llama-2-7b-hf\", \n",
    "        \"--dataset\", \"ug_clinical_guidelines\",  # Fixed dataset name\n",
    "        \"--dataset_samples\", \"10\",\n",
    "        \"--batch_size\", \"1\",\n",
    "        \"--context_length\", \"256\",\n",
    "        \"--num_epochs\", \"1\",\n",
    "        \"--save_model\", \"false\",\n",
    "        \"--log_to\", \"stdout\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🧪 Running test training...\")\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    \n",
    "    try:\n",
    "        for line in iter(process.stdout.readline, ''):\n",
    "            if line:\n",
    "                print(line.rstrip())\n",
    "        process.wait()\n",
    "        print(f\"Test completed: {process.returncode}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        process.terminate()\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Could not setup environment properly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020fe01f-ce32-418b-bf81-e0b67f337774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Converting the State Dict.ipynb'   fsdp_multi_node.sh\t tests\n",
      " LICENSE\t\t\t    hf_train.py\t\t train.py\n",
      " PROFILING.md\t\t\t    nbs\t\t\t train.sh\n",
      " README.md\t\t\t    profile.sh\t\t train_hqq_bench.sh\n",
      " __pycache__\t\t\t    profiling_utils.py\t train_sql.sh\n",
      " benchmarking\t\t\t    scripts\n",
      " benchmarks_03_2024.md\t\t    table1.sh\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc16db4d-40b6-4283-9e44-a3e98e11021e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏥 Training Uganda Clinical Model (FULL)...\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "World size: 2\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "Creating model 0\n",
      "Loading model 0\n",
      "\n",
      "Loading & Quantizing Model Shards:   0%|          | 0/2 [00:00<?, ?it/s]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "\n",
      "Loading & Quantizing Model Shards:  50%|█████     | 1/2 [00:15<00:15, 15.16s/it]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "\n",
      "\n",
      "Loading & Quantizing Model Shards: 100%|██████████| 2/2 [00:26<00:00, 12.62s/it]\n",
      "Loading & Quantizing Model Shards: 100%|██████████| 2/2 [00:26<00:00, 13.00s/it]\n",
      "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.\n",
      "  warnings.warn(  # warn only once\n",
      "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.\n",
      "  warnings.warn(  # warn only once\n",
      "Rank 0: Model created: 0.107 GiB\n",
      "Using BNB DORA 0\n",
      "Rank 0: LoRA layers added: 0.107 GiB\n",
      "Wrapping model w/ FSDP 0\n",
      "Rank 0: Wrapped model: 1.625 GiB\n",
      "Applying activation checkpointing 0\n",
      "Total Training Steps: 99\n",
      "\n",
      "  0%|          | 0/99 [00:00<?, ?it/s]\n",
      "Epoch 0, Loss 0.000:   0%|          | 0/99 [00:00<?, ?it/s]\n",
      "Epoch 0, Loss 0.000:   1%|          | 1/99 [00:08<13:29,  8.26s/it]\n",
      "Epoch 0, Loss 1.426, LR 1.00e-05:   1%|          | 1/99 [00:08<13:29,  8.26s/it]\n",
      "Epoch 0, Loss 1.426, LR 1.00e-05:   2%|▏         | 2/99 [00:10<07:31,  4.66s/it]\n",
      "Epoch 0, Loss 1.409, LR 1.00e-05:   2%|▏         | 2/99 [00:10<07:31,  4.66s/it]\n",
      "Epoch 0, Loss 1.409, LR 1.00e-05:   3%|▎         | 3/99 [00:12<05:27,  3.41s/it]\n",
      "Epoch 0, Loss 1.369, LR 1.00e-05:   3%|▎         | 3/99 [00:12<05:27,  3.41s/it]\n",
      "Epoch 0, Loss 1.369, LR 1.00e-05:   4%|▍         | 4/99 [00:14<04:27,  2.82s/it]\n",
      "Epoch 0, Loss 1.200, LR 1.00e-05:   4%|▍         | 4/99 [00:14<04:27,  2.82s/it]\n",
      "Epoch 0, Loss 1.200, LR 1.00e-05:   5%|▌         | 5/99 [00:16<03:55,  2.51s/it]\n",
      "Epoch 0, Loss 1.123, LR 1.00e-05:   5%|▌         | 5/99 [00:16<03:55,  2.51s/it]\n",
      "Epoch 0, Loss 1.123, LR 1.00e-05:   6%|▌         | 6/99 [00:18<03:37,  2.34s/it]\n",
      "Epoch 0, Loss 1.126, LR 1.00e-05:   6%|▌         | 6/99 [00:18<03:37,  2.34s/it]\n",
      "Epoch 0, Loss 1.126, LR 1.00e-05:   7%|▋         | 7/99 [00:20<03:23,  2.21s/it]\n",
      "Epoch 0, Loss 0.955, LR 1.00e-05:   7%|▋         | 7/99 [00:20<03:23,  2.21s/it]\n",
      "Epoch 0, Loss 0.955, LR 1.00e-05:   8%|▊         | 8/99 [00:22<03:20,  2.21s/it]\n",
      "Epoch 0, Loss 0.910, LR 1.00e-05:   8%|▊         | 8/99 [00:22<03:20,  2.21s/it]\n",
      "Epoch 0, Loss 0.910, LR 1.00e-05:   9%|▉         | 9/99 [00:24<03:12,  2.14s/it]\n",
      "Epoch 0, Loss 0.948, LR 1.00e-05:   9%|▉         | 9/99 [00:24<03:12,  2.14s/it]\n",
      "Epoch 0, Loss 0.948, LR 1.00e-05:  10%|█         | 10/99 [00:26<03:05,  2.08s/it]\n",
      "Epoch 0, Loss 0.674, LR 1.00e-05:  10%|█         | 10/99 [00:26<03:05,  2.08s/it]\n",
      "Epoch 0, Loss 0.674, LR 1.00e-05:  11%|█         | 11/99 [00:28<02:58,  2.03s/it]\n",
      "Epoch 0, Loss 0.956, LR 1.00e-05:  11%|█         | 11/99 [00:28<02:58,  2.03s/it]\n",
      "Epoch 0, Loss 0.956, LR 1.00e-05:  12%|█▏        | 12/99 [00:30<02:54,  2.01s/it]\n",
      "Epoch 0, Loss 0.994, LR 1.00e-05:  12%|█▏        | 12/99 [00:30<02:54,  2.01s/it]\n",
      "Epoch 0, Loss 0.994, LR 1.00e-05:  13%|█▎        | 13/99 [00:32<02:52,  2.00s/it]\n",
      "Epoch 0, Loss 0.803, LR 1.00e-05:  13%|█▎        | 13/99 [00:32<02:52,  2.00s/it]\n",
      "Epoch 0, Loss 0.803, LR 1.00e-05:  14%|█▍        | 14/99 [00:34<02:49,  2.00s/it]\n",
      "Epoch 0, Loss 0.902, LR 1.00e-05:  14%|█▍        | 14/99 [00:34<02:49,  2.00s/it]\n",
      "Epoch 0, Loss 0.902, LR 1.00e-05:  15%|█▌        | 15/99 [00:36<02:53,  2.07s/it]\n",
      "Epoch 0, Loss 1.091, LR 1.00e-05:  15%|█▌        | 15/99 [00:36<02:53,  2.07s/it]\n",
      "Epoch 0, Loss 1.091, LR 1.00e-05:  16%|█▌        | 16/99 [00:38<02:49,  2.04s/it]\n",
      "Epoch 0, Loss 0.834, LR 1.00e-05:  16%|█▌        | 16/99 [00:38<02:49,  2.04s/it]\n",
      "Epoch 0, Loss 0.834, LR 1.00e-05:  17%|█▋        | 17/99 [00:40<02:44,  2.00s/it]\n",
      "Epoch 0, Loss 1.042, LR 1.00e-05:  17%|█▋        | 17/99 [00:40<02:44,  2.00s/it]\n",
      "Epoch 0, Loss 1.042, LR 1.00e-05:  18%|█▊        | 18/99 [00:42<02:39,  1.97s/it]\n",
      "Epoch 0, Loss 0.731, LR 1.00e-05:  18%|█▊        | 18/99 [00:42<02:39,  1.97s/it]\n",
      "Epoch 0, Loss 0.731, LR 1.00e-05:  19%|█▉        | 19/99 [00:44<02:35,  1.95s/it]\n",
      "Epoch 0, Loss 1.042, LR 1.00e-05:  19%|█▉        | 19/99 [00:44<02:35,  1.95s/it]\n",
      "Epoch 0, Loss 1.042, LR 1.00e-05:  20%|██        | 20/99 [00:45<02:33,  1.94s/it]\n",
      "Epoch 0, Loss 1.062, LR 1.00e-05:  20%|██        | 20/99 [00:46<02:33,  1.94s/it]\n",
      "Epoch 0, Loss 1.062, LR 1.00e-05:  21%|██        | 21/99 [00:47<02:31,  1.95s/it]\n",
      "Epoch 0, Loss 0.817, LR 1.00e-05:  21%|██        | 21/99 [00:47<02:31,  1.95s/it]\n",
      "Epoch 0, Loss 0.817, LR 1.00e-05:  22%|██▏       | 22/99 [00:50<02:36,  2.04s/it]\n",
      "Epoch 0, Loss 0.928, LR 1.00e-05:  22%|██▏       | 22/99 [00:50<02:36,  2.04s/it]\n",
      "Epoch 0, Loss 0.928, LR 1.00e-05:  23%|██▎       | 23/99 [00:52<02:32,  2.01s/it]\n",
      "Epoch 0, Loss 1.187, LR 1.00e-05:  23%|██▎       | 23/99 [00:52<02:32,  2.01s/it]\n",
      "Epoch 0, Loss 1.187, LR 1.00e-05:  24%|██▍       | 24/99 [00:54<02:29,  1.99s/it]\n",
      "Epoch 0, Loss 1.039, LR 1.00e-05:  24%|██▍       | 24/99 [00:54<02:29,  1.99s/it]\n",
      "Epoch 0, Loss 1.039, LR 1.00e-05:  25%|██▌       | 25/99 [00:56<02:27,  1.99s/it]\n",
      "Epoch 0, Loss 0.800, LR 1.00e-05:  25%|██▌       | 25/99 [00:56<02:27,  1.99s/it]\n",
      "Epoch 0, Loss 0.800, LR 1.00e-05:  26%|██▋       | 26/99 [00:58<02:24,  1.98s/it]\n",
      "Epoch 0, Loss 0.946, LR 1.00e-05:  26%|██▋       | 26/99 [00:58<02:24,  1.98s/it]\n",
      "Epoch 0, Loss 0.946, LR 1.00e-05:  27%|██▋       | 27/99 [01:00<02:23,  1.99s/it]\n",
      "Epoch 0, Loss 1.006, LR 1.00e-05:  27%|██▋       | 27/99 [01:00<02:23,  1.99s/it]\n",
      "Epoch 0, Loss 1.006, LR 1.00e-05:  28%|██▊       | 28/99 [01:01<02:20,  1.98s/it]\n",
      "Epoch 0, Loss 0.677, LR 1.00e-05:  28%|██▊       | 28/99 [01:01<02:20,  1.98s/it]\n",
      "Epoch 0, Loss 0.677, LR 1.00e-05:  29%|██▉       | 29/99 [01:04<02:22,  2.04s/it]\n",
      "Epoch 0, Loss 1.013, LR 1.00e-05:  29%|██▉       | 29/99 [01:04<02:22,  2.04s/it]\n",
      "Epoch 0, Loss 1.013, LR 1.00e-05:  30%|███       | 30/99 [01:06<02:18,  2.01s/it]\n",
      "Epoch 0, Loss 0.918, LR 1.00e-05:  30%|███       | 30/99 [01:06<02:18,  2.01s/it]\n",
      "Epoch 0, Loss 0.918, LR 1.00e-05:  31%|███▏      | 31/99 [01:08<02:16,  2.01s/it]\n",
      "Epoch 0, Loss 0.839, LR 1.00e-05:  31%|███▏      | 31/99 [01:08<02:16,  2.01s/it]\n",
      "Epoch 0, Loss 0.839, LR 1.00e-05:  32%|███▏      | 32/99 [01:10<02:15,  2.02s/it]\n",
      "Epoch 0, Loss 1.119, LR 1.00e-05:  32%|███▏      | 32/99 [01:10<02:15,  2.02s/it]\n",
      "Epoch 0, Loss 1.119, LR 1.00e-05:  33%|███▎      | 33/99 [01:12<02:13,  2.02s/it]\n",
      "Epoch 0, Loss 0.769, LR 1.00e-05:  33%|███▎      | 33/99 [01:12<02:13,  2.02s/it]\n",
      "Epoch 1, Loss 0.769, LR 1.00e-05:  33%|███▎      | 33/99 [01:12<02:13,  2.02s/it]\n",
      "Epoch 1, Loss 0.769, LR 1.00e-05:  34%|███▍      | 34/99 [01:14<02:12,  2.03s/it]\n",
      "Epoch 1, Loss 0.613, LR 1.00e-05:  34%|███▍      | 34/99 [01:14<02:12,  2.03s/it]\n",
      "Epoch 1, Loss 0.613, LR 1.00e-05:  35%|███▌      | 35/99 [01:16<02:09,  2.02s/it]\n",
      "Epoch 1, Loss 0.661, LR 1.00e-05:  35%|███▌      | 35/99 [01:16<02:09,  2.02s/it]\n",
      "Epoch 1, Loss 0.661, LR 1.00e-05:  36%|███▋      | 36/99 [01:18<02:13,  2.13s/it]\n",
      "Epoch 1, Loss 0.629, LR 1.00e-05:  36%|███▋      | 36/99 [01:18<02:13,  2.13s/it]\n",
      "Epoch 1, Loss 0.629, LR 1.00e-05:  37%|███▋      | 37/99 [01:20<02:10,  2.10s/it]\n",
      "Epoch 1, Loss 0.638, LR 1.00e-05:  37%|███▋      | 37/99 [01:20<02:10,  2.10s/it]\n",
      "Epoch 1, Loss 0.638, LR 1.00e-05:  38%|███▊      | 38/99 [01:22<02:05,  2.05s/it]\n",
      "Epoch 1, Loss 0.596, LR 1.00e-05:  38%|███▊      | 38/99 [01:22<02:05,  2.05s/it]\n",
      "Epoch 1, Loss 0.596, LR 1.00e-05:  39%|███▉      | 39/99 [01:24<02:01,  2.02s/it]\n",
      "Epoch 1, Loss 0.618, LR 1.00e-05:  39%|███▉      | 39/99 [01:24<02:01,  2.02s/it]\n",
      "Epoch 1, Loss 0.618, LR 1.00e-05:  40%|████      | 40/99 [01:26<01:58,  2.01s/it]\n",
      "Epoch 1, Loss 0.539, LR 1.00e-05:  40%|████      | 40/99 [01:26<01:58,  2.01s/it]\n",
      "Epoch 1, Loss 0.539, LR 1.00e-05:  41%|████▏     | 41/99 [01:28<01:56,  2.02s/it]\n",
      "Epoch 1, Loss 0.418, LR 1.00e-05:  41%|████▏     | 41/99 [01:28<01:56,  2.02s/it]\n",
      "Epoch 1, Loss 0.418, LR 1.00e-05:  42%|████▏     | 42/99 [01:30<01:54,  2.01s/it]\n",
      "Epoch 1, Loss 0.477, LR 1.00e-05:  42%|████▏     | 42/99 [01:30<01:54,  2.01s/it]\n",
      "Epoch 1, Loss 0.477, LR 1.00e-05:  43%|████▎     | 43/99 [01:32<01:56,  2.09s/it]\n",
      "Epoch 1, Loss 0.350, LR 1.00e-05:  43%|████▎     | 43/99 [01:32<01:56,  2.09s/it]\n",
      "Epoch 1, Loss 0.350, LR 1.00e-05:  44%|████▍     | 44/99 [01:34<01:52,  2.04s/it]\n",
      "Epoch 1, Loss 0.469, LR 1.00e-05:  44%|████▍     | 44/99 [01:34<01:52,  2.04s/it]\n",
      "Epoch 1, Loss 0.469, LR 1.00e-05:  45%|████▌     | 45/99 [01:36<01:49,  2.02s/it]\n",
      "Epoch 1, Loss 0.488, LR 1.00e-05:  45%|████▌     | 45/99 [01:36<01:49,  2.02s/it]\n",
      "Epoch 1, Loss 0.488, LR 1.00e-05:  46%|████▋     | 46/99 [01:38<01:45,  1.99s/it]\n",
      "Epoch 1, Loss 0.434, LR 1.00e-05:  46%|████▋     | 46/99 [01:38<01:45,  1.99s/it]\n",
      "Epoch 1, Loss 0.434, LR 1.00e-05:  47%|████▋     | 47/99 [01:40<01:42,  1.98s/it]\n",
      "Epoch 1, Loss 0.382, LR 1.00e-05:  47%|████▋     | 47/99 [01:40<01:42,  1.98s/it]\n",
      "Epoch 1, Loss 0.382, LR 1.00e-05:  48%|████▊     | 48/99 [01:42<01:39,  1.95s/it]\n",
      "Epoch 1, Loss 0.577, LR 1.00e-05:  48%|████▊     | 48/99 [01:42<01:39,  1.95s/it]\n",
      "Epoch 1, Loss 0.577, LR 1.00e-05:  49%|████▉     | 49/99 [01:44<01:36,  1.93s/it]\n",
      "Epoch 1, Loss 0.414, LR 1.00e-05:  49%|████▉     | 49/99 [01:44<01:36,  1.93s/it]\n",
      "Epoch 1, Loss 0.414, LR 1.00e-05:  51%|█████     | 50/99 [01:46<01:38,  2.01s/it]\n",
      "Epoch 1, Loss 0.575, LR 1.00e-05:  51%|█████     | 50/99 [01:46<01:38,  2.01s/it]\n",
      "Epoch 1, Loss 0.575, LR 1.00e-05:  52%|█████▏    | 51/99 [01:48<01:34,  1.97s/it]\n",
      "Epoch 1, Loss 0.422, LR 1.00e-05:  52%|█████▏    | 51/99 [01:48<01:34,  1.97s/it]\n",
      "Epoch 1, Loss 0.422, LR 1.00e-05:  53%|█████▎    | 52/99 [01:50<01:31,  1.94s/it]\n",
      "Epoch 1, Loss 0.690, LR 1.00e-05:  53%|█████▎    | 52/99 [01:50<01:31,  1.94s/it]\n",
      "Epoch 1, Loss 0.690, LR 1.00e-05:  54%|█████▎    | 53/99 [01:52<01:28,  1.91s/it]\n",
      "Epoch 1, Loss 0.683, LR 1.00e-05:  54%|█████▎    | 53/99 [01:52<01:28,  1.91s/it]\n",
      "Epoch 1, Loss 0.683, LR 1.00e-05:  55%|█████▍    | 54/99 [01:54<01:25,  1.91s/it]\n",
      "Epoch 1, Loss 0.494, LR 1.00e-05:  55%|█████▍    | 54/99 [01:54<01:25,  1.91s/it]\n",
      "Epoch 1, Loss 0.494, LR 1.00e-05:  56%|█████▌    | 55/99 [01:55<01:23,  1.90s/it]\n",
      "Epoch 1, Loss 0.613, LR 1.00e-05:  56%|█████▌    | 55/99 [01:55<01:23,  1.90s/it]\n",
      "Epoch 1, Loss 0.613, LR 1.00e-05:  57%|█████▋    | 56/99 [01:57<01:21,  1.90s/it]\n",
      "Epoch 1, Loss 0.723, LR 1.00e-05:  57%|█████▋    | 56/99 [01:57<01:21,  1.90s/it]\n",
      "Epoch 1, Loss 0.723, LR 1.00e-05:  58%|█████▊    | 57/99 [01:59<01:22,  1.97s/it]\n",
      "Epoch 1, Loss 0.645, LR 1.00e-05:  58%|█████▊    | 57/99 [01:59<01:22,  1.97s/it]\n",
      "Epoch 1, Loss 0.645, LR 1.00e-05:  59%|█████▊    | 58/99 [02:01<01:20,  1.96s/it]\n",
      "Epoch 1, Loss 0.494, LR 1.00e-05:  59%|█████▊    | 58/99 [02:01<01:20,  1.96s/it]\n",
      "Epoch 1, Loss 0.494, LR 1.00e-05:  60%|█████▉    | 59/99 [02:03<01:17,  1.94s/it]\n",
      "Epoch 1, Loss 0.573, LR 1.00e-05:  60%|█████▉    | 59/99 [02:03<01:17,  1.94s/it]\n",
      "Epoch 1, Loss 0.573, LR 1.00e-05:  61%|██████    | 60/99 [02:05<01:15,  1.94s/it]\n",
      "Epoch 1, Loss 0.595, LR 1.00e-05:  61%|██████    | 60/99 [02:05<01:15,  1.94s/it]\n",
      "Epoch 1, Loss 0.595, LR 1.00e-05:  62%|██████▏   | 61/99 [02:07<01:13,  1.94s/it]\n",
      "Epoch 1, Loss 0.381, LR 1.00e-05:  62%|██████▏   | 61/99 [02:07<01:13,  1.94s/it]\n",
      "Epoch 1, Loss 0.381, LR 1.00e-05:  63%|██████▎   | 62/99 [02:09<01:11,  1.93s/it]\n",
      "Epoch 1, Loss 0.641, LR 1.00e-05:  63%|██████▎   | 62/99 [02:09<01:11,  1.93s/it]\n",
      "Epoch 1, Loss 0.641, LR 1.00e-05:  64%|██████▎   | 63/99 [02:11<01:10,  1.95s/it]\n",
      "Epoch 1, Loss 0.548, LR 1.00e-05:  64%|██████▎   | 63/99 [02:11<01:10,  1.95s/it]\n",
      "Epoch 1, Loss 0.548, LR 1.00e-05:  65%|██████▍   | 64/99 [02:13<01:11,  2.04s/it]\n",
      "Epoch 1, Loss 0.494, LR 1.00e-05:  65%|██████▍   | 64/99 [02:13<01:11,  2.04s/it]\n",
      "Epoch 1, Loss 0.494, LR 1.00e-05:  66%|██████▌   | 65/99 [02:15<01:07,  1.98s/it]\n",
      "Epoch 1, Loss 0.712, LR 1.00e-05:  66%|██████▌   | 65/99 [02:15<01:07,  1.98s/it]\n",
      "Epoch 1, Loss 0.712, LR 1.00e-05:  67%|██████▋   | 66/99 [02:17<01:04,  1.95s/it]\n",
      "Epoch 1, Loss 0.335, LR 1.00e-05:  67%|██████▋   | 66/99 [02:17<01:04,  1.95s/it]\n",
      "Epoch 2, Loss 0.335, LR 1.00e-05:  67%|██████▋   | 66/99 [02:17<01:04,  1.95s/it]\n",
      "Epoch 2, Loss 0.335, LR 1.00e-05:  68%|██████▊   | 67/99 [02:19<01:01,  1.93s/it]\n",
      "Epoch 2, Loss 0.411, LR 1.00e-05:  68%|██████▊   | 67/99 [02:19<01:01,  1.93s/it]\n",
      "Epoch 2, Loss 0.411, LR 1.00e-05:  69%|██████▊   | 68/99 [02:21<01:00,  1.96s/it]\n",
      "Epoch 2, Loss 0.455, LR 1.00e-05:  69%|██████▊   | 68/99 [02:21<01:00,  1.96s/it]\n",
      "Epoch 2, Loss 0.455, LR 1.00e-05:  70%|██████▉   | 69/99 [02:23<00:57,  1.93s/it]\n",
      "Epoch 2, Loss 0.369, LR 1.00e-05:  70%|██████▉   | 69/99 [02:23<00:57,  1.93s/it]\n",
      "Epoch 2, Loss 0.369, LR 1.00e-05:  71%|███████   | 70/99 [02:25<00:55,  1.91s/it]\n",
      "Epoch 2, Loss 0.384, LR 1.00e-05:  71%|███████   | 70/99 [02:25<00:55,  1.91s/it]\n",
      "Epoch 2, Loss 0.384, LR 1.00e-05:  72%|███████▏  | 71/99 [02:27<00:55,  1.99s/it]\n",
      "Epoch 2, Loss 0.370, LR 1.00e-05:  72%|███████▏  | 71/99 [02:27<00:55,  1.99s/it]\n",
      "Epoch 2, Loss 0.370, LR 1.00e-05:  73%|███████▎  | 72/99 [02:29<00:53,  1.97s/it]\n",
      "Epoch 2, Loss 0.396, LR 1.00e-05:  73%|███████▎  | 72/99 [02:29<00:53,  1.97s/it]\n",
      "Epoch 2, Loss 0.396, LR 1.00e-05:  74%|███████▎  | 73/99 [02:31<00:50,  1.94s/it]\n",
      "Epoch 2, Loss 0.337, LR 1.00e-05:  74%|███████▎  | 73/99 [02:31<00:50,  1.94s/it]\n",
      "Epoch 2, Loss 0.337, LR 1.00e-05:  75%|███████▍  | 74/99 [02:33<00:48,  1.92s/it]\n",
      "Epoch 2, Loss 0.206, LR 1.00e-05:  75%|███████▍  | 74/99 [02:33<00:48,  1.92s/it]\n",
      "Epoch 2, Loss 0.206, LR 1.00e-05:  76%|███████▌  | 75/99 [02:34<00:45,  1.91s/it]\n",
      "Epoch 2, Loss 0.247, LR 1.00e-05:  76%|███████▌  | 75/99 [02:34<00:45,  1.91s/it]\n",
      "Epoch 2, Loss 0.247, LR 1.00e-05:  77%|███████▋  | 76/99 [02:36<00:43,  1.91s/it]\n",
      "Epoch 2, Loss 0.183, LR 1.00e-05:  77%|███████▋  | 76/99 [02:36<00:43,  1.91s/it]\n",
      "Epoch 2, Loss 0.183, LR 1.00e-05:  78%|███████▊  | 77/99 [02:38<00:41,  1.90s/it]\n",
      "Epoch 2, Loss 0.236, LR 1.00e-05:  78%|███████▊  | 77/99 [02:38<00:41,  1.90s/it]\n",
      "Epoch 2, Loss 0.236, LR 1.00e-05:  79%|███████▉  | 78/99 [02:40<00:42,  2.01s/it]\n",
      "Epoch 2, Loss 0.220, LR 1.00e-05:  79%|███████▉  | 78/99 [02:40<00:42,  2.01s/it]\n",
      "Epoch 2, Loss 0.220, LR 1.00e-05:  80%|███████▉  | 79/99 [02:42<00:40,  2.01s/it]\n",
      "Epoch 2, Loss 0.186, LR 1.00e-05:  80%|███████▉  | 79/99 [02:42<00:40,  2.01s/it]\n",
      "Epoch 2, Loss 0.186, LR 1.00e-05:  81%|████████  | 80/99 [02:44<00:37,  1.99s/it]\n",
      "Epoch 2, Loss 0.251, LR 1.00e-05:  81%|████████  | 80/99 [02:44<00:37,  1.99s/it]\n",
      "Epoch 2, Loss 0.251, LR 1.00e-05:  82%|████████▏ | 81/99 [02:46<00:35,  1.95s/it]\n",
      "Epoch 2, Loss 0.315, LR 1.00e-05:  82%|████████▏ | 81/99 [02:46<00:35,  1.95s/it]\n",
      "Epoch 2, Loss 0.315, LR 1.00e-05:  83%|████████▎ | 82/99 [02:48<00:33,  1.94s/it]\n",
      "Epoch 2, Loss 0.147, LR 1.00e-05:  83%|████████▎ | 82/99 [02:48<00:33,  1.94s/it]\n",
      "Epoch 2, Loss 0.147, LR 1.00e-05:  84%|████████▍ | 83/99 [02:50<00:31,  1.94s/it]\n",
      "Epoch 2, Loss 0.208, LR 1.00e-05:  84%|████████▍ | 83/99 [02:50<00:31,  1.94s/it]\n",
      "Epoch 2, Loss 0.208, LR 1.00e-05:  85%|████████▍ | 84/99 [02:52<00:29,  1.94s/it]\n",
      "Epoch 2, Loss 0.187, LR 1.00e-05:  85%|████████▍ | 84/99 [02:52<00:29,  1.94s/it]\n",
      "Epoch 2, Loss 0.187, LR 1.00e-05:  86%|████████▌ | 85/99 [02:54<00:28,  2.03s/it]\n",
      "Epoch 2, Loss 0.394, LR 1.00e-05:  86%|████████▌ | 85/99 [02:54<00:28,  2.03s/it]\n",
      "Epoch 2, Loss 0.394, LR 1.00e-05:  87%|████████▋ | 86/99 [02:56<00:25,  1.99s/it]\n",
      "Epoch 2, Loss 0.326, LR 1.00e-05:  87%|████████▋ | 86/99 [02:56<00:25,  1.99s/it]\n",
      "Epoch 2, Loss 0.326, LR 1.00e-05:  88%|████████▊ | 87/99 [02:58<00:23,  1.97s/it]\n",
      "Epoch 2, Loss 0.227, LR 1.00e-05:  88%|████████▊ | 87/99 [02:58<00:23,  1.97s/it]\n",
      "Epoch 2, Loss 0.227, LR 1.00e-05:  89%|████████▉ | 88/99 [03:00<00:21,  1.96s/it]\n",
      "Epoch 2, Loss 0.256, LR 1.00e-05:  89%|████████▉ | 88/99 [03:00<00:21,  1.96s/it]\n",
      "Epoch 2, Loss 0.256, LR 1.00e-05:  90%|████████▉ | 89/99 [03:02<00:19,  1.96s/it]\n",
      "Epoch 2, Loss 0.334, LR 1.00e-05:  90%|████████▉ | 89/99 [03:02<00:19,  1.96s/it]\n",
      "Epoch 2, Loss 0.334, LR 1.00e-05:  91%|█████████ | 90/99 [03:04<00:17,  1.94s/it]\n",
      "Epoch 2, Loss 0.330, LR 1.00e-05:  91%|█████████ | 90/99 [03:04<00:17,  1.94s/it]\n",
      "Epoch 2, Loss 0.330, LR 1.00e-05:  92%|█████████▏| 91/99 [03:06<00:15,  1.94s/it]\n",
      "Epoch 2, Loss 0.259, LR 1.00e-05:  92%|█████████▏| 91/99 [03:06<00:15,  1.94s/it]\n",
      "Epoch 2, Loss 0.259, LR 1.00e-05:  93%|█████████▎| 92/99 [03:08<00:14,  2.02s/it]\n",
      "Epoch 2, Loss 0.283, LR 1.00e-05:  93%|█████████▎| 92/99 [03:08<00:14,  2.02s/it]\n",
      "Epoch 2, Loss 0.283, LR 1.00e-05:  94%|█████████▍| 93/99 [03:11<00:13,  2.17s/it]\n",
      "Epoch 2, Loss 0.271, LR 1.00e-05:  94%|█████████▍| 93/99 [03:11<00:13,  2.17s/it]\n",
      "Epoch 2, Loss 0.271, LR 1.00e-05:  95%|█████████▍| 94/99 [03:21<00:23,  4.62s/it]\n",
      "Epoch 2, Loss 0.180, LR 1.00e-05:  95%|█████████▍| 94/99 [03:21<00:23,  4.62s/it]\n",
      "Epoch 2, Loss 0.180, LR 1.00e-05:  96%|█████████▌| 95/99 [03:23<00:15,  3.83s/it]\n",
      "Epoch 2, Loss 0.325, LR 1.00e-05:  96%|█████████▌| 95/99 [03:23<00:15,  3.83s/it]\n",
      "Epoch 2, Loss 0.325, LR 1.00e-05:  97%|█████████▋| 96/99 [03:25<00:09,  3.27s/it]\n",
      "Epoch 2, Loss 0.280, LR 1.00e-05:  97%|█████████▋| 96/99 [03:25<00:09,  3.27s/it]\n",
      "Epoch 2, Loss 0.280, LR 1.00e-05:  98%|█████████▊| 97/99 [03:27<00:05,  2.90s/it]\n",
      "Epoch 2, Loss 0.228, LR 1.00e-05:  98%|█████████▊| 97/99 [03:27<00:05,  2.90s/it]\n",
      "Epoch 2, Loss 0.228, LR 1.00e-05:  99%|█████████▉| 98/99 [03:29<00:02,  2.65s/it]\n",
      "Epoch 2, Loss 0.380, LR 1.00e-05:  99%|█████████▉| 98/99 [03:29<00:02,  2.65s/it]\n",
      "Epoch 2, Loss 0.380, LR 1.00e-05: 100%|██████████| 99/99 [03:31<00:00,  2.56s/it]\n",
      "Epoch 2, Loss 0.106, LR 1.00e-05: 100%|██████████| 99/99 [03:31<00:00,  2.56s/it]/usr/local/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .\n",
      "  warnings.warn(\n",
      "Using BNB DORA 1\n",
      "\n",
      "Epoch 2, Loss 0.106, LR 1.00e-05: 100%|██████████| 99/99 [03:33<00:00,  2.16s/it]\n",
      "Finished training 0\n",
      "CUDA event elapsed time: 211.839921875 sec\n",
      "time_taken: 211.839921875\n",
      "Rank 0: Before forward: 1.62 GiB\n",
      "Rank 0: After forward: 2.46 GiB\n",
      "Rank 0: After backward: 2.64 GiB\n",
      "Rank 0: Peak allocated memory: 1.41 GiB\n",
      "Rank 0: Peak reserved memory:  2.65 GiB\n",
      "Saving trained LoRA weights.\n",
      "Done 0\n",
      "Training completed: 0\n",
      "🎉 Model saved successfully!\n",
      "📁 Saved files:\n",
      "  📄 model_state_dict.safetensors\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set environment\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "# FULL TRAINING with model saving\n",
    "cmd = [\n",
    "    \"python\", \"train.py\",\n",
    "    \"--train_type\", \"bnb_dora\",\n",
    "    \"--model_name\", \"meta-llama/Llama-2-7b-hf\", \n",
    "    \"--dataset\", \"ug_clinical_guidelines\",\n",
    "    \"--dataset_samples\", \"130\",  # Use all your data\n",
    "    \"--batch_size\", \"2\",\n",
    "    \"--context_length\", \"512\",   # Longer context for medical text\n",
    "    \"--precision\", \"bf16\",\n",
    "    \"--num_epochs\", \"3\",         # More epochs for better training\n",
    "    \"--save_model\", \"true\",      # 🔥 SAVE THE MODEL\n",
    "    \"--output_dir\", \"./uganda_clinical_model\",  # Where to save\n",
    "    \"--log_to\", \"stdout\"\n",
    "]\n",
    "\n",
    "print(\"🏥 Training Uganda Clinical Model (FULL)...\")\n",
    "process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "\n",
    "try:\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        if line:\n",
    "            print(line.rstrip())\n",
    "    process.wait()\n",
    "    print(f\"Training completed: {process.returncode}\")\n",
    "    \n",
    "    # Check if model was saved\n",
    "    if os.path.exists(\"uganda_clinical_model\"):\n",
    "        print(\"🎉 Model saved successfully!\")\n",
    "        print(\"📁 Saved files:\")\n",
    "        for f in os.listdir(\"uganda_clinical_model\"):\n",
    "            print(f\"  📄 {f}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")\n",
    "    process.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4dcfe1-5b16-418f-ace3-f519f86dc3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Packaging model for download...\n",
      "✅ Model packaged as uganda_clinical_qdora_model.zip\n",
      "📊 File size: 217.9 MB\n",
      "💾 You can download this file from the Jupyter file browser\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "def download_model():\n",
    "    \"\"\"Package the model for download\"\"\"\n",
    "    if os.path.exists(\"uganda_clinical_model\"):\n",
    "        print(\"📦 Packaging model for download...\")\n",
    "        \n",
    "        # Create a zip file\n",
    "        with zipfile.ZipFile(\"uganda_clinical_qdora_model.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files in os.walk(\"uganda_clinical_model\"):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    zipf.write(file_path, file_path)\n",
    "        \n",
    "        print(\"✅ Model packaged as uganda_clinical_qdora_model.zip\")\n",
    "        print(f\"📊 File size: {os.path.getsize('uganda_clinical_qdora_model.zip') / 1024 / 1024:.1f} MB\")\n",
    "        \n",
    "        # In Jupyter, this will be available for download\n",
    "        print(\"💾 You can download this file from the Jupyter file browser\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No model directory found. Run training with --save_model true first\")\n",
    "\n",
    "download_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e76b987c-0dac-470b-96fa-2ef2a2e07e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading trained model...\n",
      "❌ Error loading model: Unrecognized model in ./uganda_clinical_model. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    \"\"\"Test the trained model\"\"\"\n",
    "    \n",
    "    # Load the model for inference\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "    \n",
    "    model_path = \"./uganda_clinical_model\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"❌ Model not found. Train with --save_model true first\")\n",
    "        return\n",
    "    \n",
    "    print(\"🔄 Loading trained model...\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "        \n",
    "        # Test with a medical question\n",
    "        test_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "I have fever, general body weakness, joint paints and have been getting by mosquitoes often. what could be the cause ?\n",
    "\n",
    "### Response:\"\"\"\n",
    "        \n",
    "        print(\"\\n🧪 Testing model...\")\n",
    "        print(\"Question: What are the symptoms of malaria?\")\n",
    "        print(\"\\nModel Response:\")\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=inputs.input_ids.shape[1] + 150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extract just the response part\n",
    "        response_only = response.split(\"### Response:\")[-1].strip()\n",
    "        print(response_only)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "\n",
    "# Run after you've saved the model\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b79225ac-e989-4b6a-9217-02f037372c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Files in uganda_clinical_model:\n",
      "  📄 model_state_dict.safetensors (275.4 MB)\n",
      "❌ Missing: adapter_config.json\n",
      "❌ Missing: adapter_model.bin\n",
      "❌ Missing: adapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def inspect_saved_model():\n",
    "    \"\"\"Check what files were actually saved\"\"\"\n",
    "    model_dir = \"./uganda_clinical_model\"\n",
    "    \n",
    "    if os.path.exists(model_dir):\n",
    "        print(\"📁 Files in uganda_clinical_model:\")\n",
    "        for file in os.listdir(model_dir):\n",
    "            file_path = os.path.join(model_dir, file)\n",
    "            size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "            print(f\"  📄 {file} ({size:.1f} MB)\")\n",
    "        \n",
    "        # Check for specific files\n",
    "        expected_files = [\"adapter_config.json\", \"adapter_model.bin\", \"adapter_model.safetensors\"]\n",
    "        for expected in expected_files:\n",
    "            if os.path.exists(os.path.join(model_dir, expected)):\n",
    "                print(f\"✅ Found: {expected}\")\n",
    "            else:\n",
    "                print(f\"❌ Missing: {expected}\")\n",
    "    else:\n",
    "        print(\"❌ Model directory not found\")\n",
    "\n",
    "inspect_saved_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926eebe-529f-4271-8a0d-0b9ea24c60e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
