<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Efficient Finetuning of Llama 3 70B with FSDP QDora – fsdp_ucg</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">FSDP QDora Tutorial</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./fsdp_qdora_ucg_v1.html" rel="" target="">
 <span class="menu-text">FSDP QDora Tutorial</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./sd_ulmfit.html" rel="" target="">
 <span class="menu-text">Text Transfer Learning with ULMFit - Medical LLM V1</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/rubanzasilva" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://flexiblefunctions.com" rel="" target="">
 <span class="menu-text">Flexible Functions</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">



<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> subprocess</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> zipfile</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ssl</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an SSL context that doesn't verify certificates</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>ssl_context <span class="op">=</span> ssl.create_default_context()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>ssl_context.check_hostname <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>ssl_context.verify_mode <span class="op">=</span> ssl.CERT_NONE</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Install packages first</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"🔧 Installing Python packages..."</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>packages <span class="op">=</span> [</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"torch"</span>, <span class="st">"torchvision"</span>, <span class="st">"torchaudio"</span>, </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"transformers"</span>, <span class="st">"datasets"</span>, <span class="st">"accelerate"</span>, <span class="st">"peft"</span>,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"bitsandbytes&gt;=0.43.0"</span>, <span class="st">"safetensors"</span>, <span class="st">"fastcore"</span>, <span class="st">"requests"</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> package <span class="kw">in</span> packages:</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        subprocess.check_call([sys.executable, <span class="st">"-m"</span>, <span class="st">"pip"</span>, <span class="st">"install"</span>, <span class="st">"--user"</span>, package], </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>                            stdout<span class="op">=</span>subprocess.DEVNULL, stderr<span class="op">=</span>subprocess.DEVNULL)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"✅ Installed </span><span class="sc">{</span>package<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span>:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"❌ Failed to install </span><span class="sc">{</span>package<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Download with SSL verification disabled</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">📥 Downloading repository..."</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://github.com/AnswerDotAI/fsdp_qlora/archive/refs/heads/main.zip"</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use urllib with disabled SSL verification</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    urllib.request.urlretrieve(url, <span class="st">"fsdp_qlora.zip"</span>, context<span class="op">=</span>ssl_context)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✅ Downloaded using urllib"</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"❌ urllib failed: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fallback to requests</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> requests</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"🔄 Trying with requests..."</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> requests.get(url, verify<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"fsdp_qlora.zip"</span>, <span class="st">"wb"</span>) <span class="im">as</span> f:</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>            f.write(response.content)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"✅ Downloaded using requests"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e2:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"❌ requests also failed: </span><span class="sc">{</span>e2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract if download succeeded</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(<span class="st">"fsdp_qlora.zip"</span>):</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"📂 Extracting repository..."</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Clean up existing directory</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.path.exists(<span class="st">"fsdp_qlora"</span>):</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> shutil</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        shutil.rmtree(<span class="st">"fsdp_qlora"</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> zipfile.ZipFile(<span class="st">"fsdp_qlora.zip"</span>, <span class="st">'r'</span>) <span class="im">as</span> zip_ref:</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        zip_ref.extractall(<span class="st">"."</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    os.rename(<span class="st">"fsdp_qlora-main"</span>, <span class="st">"fsdp_qlora"</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    os.remove(<span class="st">"fsdp_qlora.zip"</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✅ Setup complete! Repository is in ./fsdp_qlora"</span>)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Verify</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.path.exists(<span class="st">"fsdp_qlora/train.py"</span>):</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"🚀 train.py found - ready to train!"</span>)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"❌ train.py not found"</span>)</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"❌ Download failed completely"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>🔧 Installing Python packages...
✅ Installed torch
✅ Installed torchvision
✅ Installed torchaudio
✅ Installed transformers
✅ Installed datasets
✅ Installed accelerate
✅ Installed peft
✅ Installed bitsandbytes&gt;=0.43.0
✅ Installed safetensors
✅ Installed fastcore
✅ Installed requests

📥 Downloading repository...
❌ urllib failed: urlretrieve() got an unexpected keyword argument 'context'
🔄 Trying with requests...
✅ Downloaded using requests
📂 Extracting repository...
✅ Setup complete! Repository is in ./fsdp_qlora
🚀 train.py found - ready to train!</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'github.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
  warnings.warn(
/usr/local/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'codeload.github.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
  warnings.warn(</code></pre>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install huggingface_hub</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: huggingface_hub in /root/.local/lib/python3.11/site-packages (0.33.4)
Requirement already satisfied: filelock in /root/.local/lib/python3.11/site-packages (from huggingface_hub) (3.18.0)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /root/.local/lib/python3.11/site-packages (from huggingface_hub) (2025.3.0)
Requirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (25.0)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)
Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (2.32.4)
Requirement already satisfied: tqdm&gt;=4.42.1 in /root/.local/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)
Requirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.2 in /root/.local/lib/python3.11/site-packages (from huggingface_hub) (1.1.5)
Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (3.4.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (2.5.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests-&gt;huggingface_hub) (2024.8.30)
Note: you may need to restart the kernel to use updated packages.</code></pre>
</div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="3">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install llama<span class="op">-</span>recipes fastcore <span class="st">"transformers!=4.38.*,!=4.39.*"</span> <span class="op">--</span>extra<span class="op">-</span>index<span class="op">-</span>url https:<span class="op">//</span>download.pytorch.org<span class="op">/</span>whl<span class="op">/</span>test<span class="op">/</span>cu118</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/test/cu118
Collecting llama-recipes
  Downloading llama_recipes-0.0.5.post2-py3-none-any.whl.metadata (5.0 kB)
Requirement already satisfied: fastcore in /root/.local/lib/python3.11/site-packages (1.8.5)
Requirement already satisfied: transformers!=4.38.*,!=4.39.* in /root/.local/lib/python3.11/site-packages (4.53.2)
Collecting llama-cookbook==0.0.5.post1 (from llama-recipes)
  Downloading llama_cookbook-0.0.5.post1-py3-none-any.whl.metadata (5.8 kB)
Requirement already satisfied: accelerate in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.9.0)
Collecting appdirs (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)
Requirement already satisfied: bitsandbytes in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.46.1)
Collecting black (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)
Collecting chardet (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)
Collecting codeshield (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading codeshield-1.0.1-py3-none-any.whl.metadata (5.2 kB)
Requirement already satisfied: datasets in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1-&gt;llama-recipes) (4.0.0)
Collecting evaluate (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)
Collecting fire (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading fire-0.7.0.tar.gz (87 kB)
  Preparing metadata (setup.py) ... done
Collecting gradio (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading gradio-5.38.0-py3-none-any.whl.metadata (16 kB)
Collecting loralib (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)
Collecting markupsafe==2.0.1 (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading MarkupSafe-2.0.1.tar.gz (18 kB)
  Preparing metadata (setup.py) ... done
Collecting matplotlib (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)
Collecting openai (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading openai-1.97.0-py3-none-any.whl.metadata (29 kB)
Collecting optimum (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading optimum-1.26.1-py3-none-any.whl.metadata (16 kB)
Requirement already satisfied: peft in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.16.0)
Collecting py7zr (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading py7zr-1.0.0-py3-none-any.whl.metadata (17 kB)
Collecting pyyaml==6.0.1 (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading https://download.pytorch.org/whl/test/PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 757.7/757.7 kB 3.6 MB/s eta 0:00:00
Collecting rouge-score (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading rouge_score-0.1.2.tar.gz (17 kB)
  Preparing metadata (setup.py) ... done
Collecting scipy (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)
Collecting sentence-transformers (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)
Collecting sentencepiece (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading https://download.pytorch.org/whl/test/sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 5.1 MB/s eta 0:00:00a 0:00:01
Collecting tabulate (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)
Requirement already satisfied: torch&gt;=2.2 in /root/.local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.7.1)
Requirement already satisfied: typing-extensions&gt;=4.8.0 in /usr/local/lib/python3.11/site-packages (from llama-cookbook==0.0.5.post1-&gt;llama-recipes) (4.12.2)
Collecting unstructured[pdf] (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading unstructured-0.18.9-py3-none-any.whl.metadata (24 kB)
Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from fastcore) (25.0)
Requirement already satisfied: filelock in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (3.18.0)
Requirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.30.0 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (0.33.4)
Requirement already satisfied: numpy&gt;=1.17 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (2.3.1)
Requirement already satisfied: regex!=2019.12.17 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (2024.11.6)
Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (2.32.4)
Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (0.21.2)
Requirement already satisfied: safetensors&gt;=0.4.3 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (0.5.3)
Requirement already satisfied: tqdm&gt;=4.27 in /root/.local/lib/python3.11/site-packages (from transformers!=4.38.*,!=4.39.*) (4.67.1)
Requirement already satisfied: fsspec&gt;=2023.5.0 in /root/.local/lib/python3.11/site-packages (from huggingface-hub&lt;1.0,&gt;=0.30.0-&gt;transformers!=4.38.*,!=4.39.*) (2025.3.0)
Requirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.2 in /root/.local/lib/python3.11/site-packages (from huggingface-hub&lt;1.0,&gt;=0.30.0-&gt;transformers!=4.38.*,!=4.39.*) (1.1.5)
Requirement already satisfied: sympy&gt;=1.13.3 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.14.0)
Requirement already satisfied: networkx in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (3.5)
Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (3.1.6)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (12.6.77)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (12.6.77)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (12.6.80)
Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (9.5.1.17)
Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (12.6.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (11.3.0.4)
Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (10.3.7.77)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (11.7.1.2)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (12.5.4.2)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.6.3)
Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.26.2)
Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (12.6.77)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (12.6.85)
Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.11.1.6)
Requirement already satisfied: triton==3.3.1 in /root/.local/lib/python3.11/site-packages (from torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (3.3.1)
Requirement already satisfied: setuptools&gt;=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.3.1-&gt;torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (68.1.2)
Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /root/.local/lib/python3.11/site-packages (from sympy&gt;=1.13.3-&gt;torch&gt;=2.2-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.3.0)
Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from accelerate-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (7.0.0)
Collecting click&gt;=8.0.0 (from black-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)
Collecting mypy-extensions&gt;=0.4.3 (from black-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)
Collecting pathspec&gt;=0.9.0 (from black-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
Requirement already satisfied: platformdirs&gt;=2 in /usr/local/lib/python3.11/site-packages (from black-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (4.3.8)
Requirement already satisfied: ipython&gt;=7.8.0 in /usr/local/lib/python3.11/site-packages (from black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (9.4.0)
Collecting tokenize-rt&gt;=3.2.0 (from black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading tokenize_rt-6.2.0-py2.py3-none-any.whl.metadata (4.0 kB)
Requirement already satisfied: decorator in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (5.2.1)
Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.1.1)
Requirement already satisfied: jedi&gt;=0.16 in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.19.2)
Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.1.7)
Requirement already satisfied: pexpect&gt;4.3 in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (4.9.0)
Requirement already satisfied: prompt_toolkit&lt;3.1.0,&gt;=3.0.41 in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (3.0.51)
Requirement already satisfied: pygments&gt;=2.4.0 in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.19.2)
Requirement already satisfied: stack_data in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.6.3)
Requirement already satisfied: traitlets&gt;=5.13.0 in /usr/local/lib/python3.11/site-packages (from ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (5.14.3)
Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/site-packages (from prompt_toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.2.13)
Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.4 in /usr/local/lib/python3.11/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.8.4)
Requirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.7.0)
Collecting semgrep&gt;1.68 (from codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading semgrep-1.128.1-cp39.cp310.cp311.py39.py310.py311-none-musllinux_1_0_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)
Requirement already satisfied: attrs&gt;=21.3 in /usr/local/lib/python3.11/site-packages (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (24.2.0)
Collecting boltons~=21.0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading boltons-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)
Collecting click-option-group~=0.5 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading click_option_group-0.5.7-py3-none-any.whl.metadata (5.8 kB)
Collecting click&gt;=8.0.0 (from black-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)
Collecting colorama~=0.4.0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading https://download.pytorch.org/whl/test/colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Requirement already satisfied: defusedxml~=0.7.1 in /usr/local/lib/python3.11/site-packages (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.7.1)
Collecting exceptiongroup~=1.2.0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)
Collecting glom~=22.1 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading glom-22.1.0-py2.py3-none-any.whl.metadata (4.9 kB)
Requirement already satisfied: jsonschema~=4.6 in /usr/local/lib/python3.11/site-packages (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (4.24.0)
Collecting opentelemetry-api~=1.25.0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)
Collecting opentelemetry-sdk~=1.25.0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)
Collecting opentelemetry-exporter-otlp-proto-http~=1.25.0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl.metadata (2.2 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.56b0-py3-none-any.whl.metadata (2.6 kB)
Collecting peewee~=3.14 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading peewee-3.18.2.tar.gz (949 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 949.2/949.2 kB 68.9 MB/s eta 0:00:00
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting rich~=13.5.2 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading rich-13.5.3-py3-none-any.whl.metadata (18 kB)
Collecting ruamel.yaml&gt;=0.18.5 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)
Collecting tomli~=2.0.1 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading tomli-2.0.2-py3-none-any.whl.metadata (10.0 kB)
Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/site-packages (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.5.0)
Collecting wcmatch~=8.3 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading wcmatch-8.5.2-py3-none-any.whl.metadata (4.8 kB)
Collecting face&gt;=20.1.0 (from glom~=22.1-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading face-24.0.0-py3-none-any.whl.metadata (1.1 kB)
Requirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /usr/local/lib/python3.11/site-packages (from jsonschema~=4.6-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2025.4.1)
Requirement already satisfied: referencing&gt;=0.28.4 in /usr/local/lib/python3.11/site-packages (from jsonschema~=4.6-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.36.2)
Requirement already satisfied: rpds-py&gt;=0.7.1 in /usr/local/lib/python3.11/site-packages (from jsonschema~=4.6-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.26.0)
Collecting deprecated&gt;=1.2.6 (from opentelemetry-api~=1.25.0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)
Collecting importlib-metadata&lt;=7.1,&gt;=6.0 (from opentelemetry-api~=1.25.0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading https://download.pytorch.org/whl/test/importlib_metadata-7.1.0-py3-none-any.whl (24 kB)
Collecting zipp&gt;=0.5 (from importlib-metadata&lt;=7.1,&gt;=6.0-&gt;opentelemetry-api~=1.25.0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)
Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-http~=1.25.0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)
Collecting opentelemetry-exporter-otlp-proto-common==1.25.0 (from opentelemetry-exporter-otlp-proto-http~=1.25.0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl.metadata (1.7 kB)
Collecting opentelemetry-proto==1.25.0 (from opentelemetry-exporter-otlp-proto-http~=1.25.0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_proto-1.25.0-py3-none-any.whl.metadata (2.2 kB)
Collecting protobuf&lt;5.0,&gt;=3.19 (from opentelemetry-proto==1.25.0-&gt;opentelemetry-exporter-otlp-proto-http~=1.25.0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)
Collecting opentelemetry-instrumentation==0.56b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.56b0-py3-none-any.whl.metadata (6.7 kB)
Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)
Collecting opentelemetry-util-http==0.56b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.56b0-py3-none-any.whl.metadata (2.6 kB)
Collecting wrapt&lt;2.0.0,&gt;=1.0.0 (from opentelemetry-instrumentation==0.56b0-&gt;opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)
INFO: pip is looking at multiple versions of opentelemetry-semantic-conventions to determine which version is compatible with other requirements. This could take a while.
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.55b1-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation==0.55b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.55b1-py3-none-any.whl.metadata (6.7 kB)
Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.55b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.55b1-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.55b0-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation==0.55b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.55b0-py3-none-any.whl.metadata (6.7 kB)
Collecting opentelemetry-semantic-conventions==0.55b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.55b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.55b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.55b0-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.54b1-py3-none-any.whl.metadata (2.7 kB)
Collecting opentelemetry-instrumentation==0.54b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.54b1-py3-none-any.whl.metadata (6.8 kB)
Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.54b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.54b1-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.54b0-py3-none-any.whl.metadata (2.7 kB)
Collecting opentelemetry-instrumentation==0.54b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.54b0-py3-none-any.whl.metadata (6.8 kB)
Collecting opentelemetry-semantic-conventions==0.54b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.54b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.54b0-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.53b1-py3-none-any.whl.metadata (2.7 kB)
Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)
Collecting opentelemetry-semantic-conventions==0.53b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.53b1-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.53b0-py3-none-any.whl.metadata (2.7 kB)
Collecting opentelemetry-instrumentation==0.53b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.53b0-py3-none-any.whl.metadata (6.8 kB)
Collecting opentelemetry-semantic-conventions==0.53b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.53b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.53b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.53b0-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.52b1-py3-none-any.whl.metadata (2.7 kB)
Collecting opentelemetry-instrumentation==0.52b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.52b1-py3-none-any.whl.metadata (6.8 kB)
Collecting opentelemetry-semantic-conventions==0.52b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.52b1-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.52b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.52b1-py3-none-any.whl.metadata (2.6 kB)
INFO: pip is still looking at multiple versions of opentelemetry-semantic-conventions to determine which version is compatible with other requirements. This could take a while.
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.52b0-py3-none-any.whl.metadata (2.7 kB)
Collecting opentelemetry-instrumentation==0.52b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.52b0-py3-none-any.whl.metadata (6.8 kB)
Collecting opentelemetry-semantic-conventions==0.52b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.52b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.52b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.52b0-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.51b0-py3-none-any.whl.metadata (2.7 kB)
Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)
Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.50b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)
Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)
Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.49b2-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)
Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)
Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.49b1-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation==0.49b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)
Collecting opentelemetry-semantic-conventions==0.49b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl.metadata (2.4 kB)
Collecting opentelemetry-util-http==0.49b1 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl.metadata (2.5 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.49b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation==0.49b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)
Collecting opentelemetry-semantic-conventions==0.49b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl.metadata (2.4 kB)
Collecting opentelemetry-util-http==0.49b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.49b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.48b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)
Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)
Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.47b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)
Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)
Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation-requests~=0.46b0 (from semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation_requests-0.46b0-py3-none-any.whl.metadata (2.5 kB)
Collecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)
Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)
Collecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-requests~=0.46b0-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)
Requirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/site-packages (from requests-&gt;transformers!=4.38.*,!=4.39.*) (3.4.2)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/site-packages (from requests-&gt;transformers!=4.38.*,!=4.39.*) (3.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests-&gt;transformers!=4.38.*,!=4.39.*) (2024.8.30)
Collecting markdown-it-py&gt;=2.2.0 (from rich~=13.5.2-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
Collecting bracex&gt;=2.1.1 (from wcmatch~=8.3-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading bracex-2.6-py3-none-any.whl.metadata (3.6 kB)
Collecting mdurl~=0.1 (from markdown-it-py&gt;=2.2.0-&gt;rich~=13.5.2-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
Collecting ruamel.yaml.clib&gt;=0.2.7 (from ruamel.yaml&gt;=0.18.5-&gt;semgrep&gt;1.68-&gt;codeshield-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)
Requirement already satisfied: pyarrow&gt;=15.0.0 in /root/.local/lib/python3.11/site-packages (from datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (21.0.0)
Requirement already satisfied: dill&lt;0.3.9,&gt;=0.3.0 in /root/.local/lib/python3.11/site-packages (from datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.3.8)
Requirement already satisfied: pandas in /root/.local/lib/python3.11/site-packages (from datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.3.1)
Requirement already satisfied: xxhash in /root/.local/lib/python3.11/site-packages (from datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (3.5.0)
Requirement already satisfied: multiprocess&lt;0.70.17 in /root/.local/lib/python3.11/site-packages (from datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.70.16)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (3.10.8)
Requirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.4.3)
Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.3.1)
Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.4.1)
Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (6.1.0)
Requirement already satisfied: yarl&lt;2.0,&gt;=1.12.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.3.0,&gt;=2023.1.0-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.13.1)
Collecting termcolor (from fire-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)
Collecting aiofiles&lt;25.0,&gt;=22.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)
Requirement already satisfied: anyio&lt;5.0,&gt;=3.0 in /usr/local/lib/python3.11/site-packages (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (4.9.0)
Collecting brotli&gt;=1.1.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)
Collecting fastapi&lt;1.0,&gt;=0.115.2 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)
Collecting ffmpy (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)
Collecting gradio-client==1.11.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading gradio_client-1.11.0-py3-none-any.whl.metadata (7.1 kB)
Collecting groovy~=0.1 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)
Requirement already satisfied: httpx&lt;1.0,&gt;=0.24.1 in /usr/local/lib/python3.11/site-packages (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.28.1)
Collecting orjson~=3.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading orjson-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (42 kB)
Requirement already satisfied: pillow&lt;12.0,&gt;=8.0 in /root/.local/lib/python3.11/site-packages (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (11.3.0)
Collecting pydantic&lt;2.12,&gt;=2.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)
Collecting pydub (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting python-multipart&gt;=0.0.18 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)
Collecting ruff&gt;=0.9.3 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading ruff-0.12.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)
Collecting safehttpx&lt;0.2.0,&gt;=0.1.6 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)
Collecting semantic-version~=2.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)
Collecting starlette&lt;1.0,&gt;=0.40.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading starlette-0.47.1-py3-none-any.whl.metadata (6.2 kB)
Collecting tomlkit&lt;0.14.0,&gt;=0.12.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)
Collecting typer&lt;1.0,&gt;=0.12 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)
Collecting uvicorn&gt;=0.14.0 (from gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)
Collecting websockets&lt;16.0,&gt;=10.0 (from gradio-client==1.11.0-&gt;gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Requirement already satisfied: sniffio&gt;=1.1 in /usr/local/lib/python3.11/site-packages (from anyio&lt;5.0,&gt;=3.0-&gt;gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.3.1)
Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx&lt;1.0,&gt;=0.24.1-&gt;gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.0.9)
Requirement already satisfied: h11&gt;=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*-&gt;httpx&lt;1.0,&gt;=0.24.1-&gt;gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.16.0)
Requirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in /root/.local/lib/python3.11/site-packages (from pandas-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2025.2)
Requirement already satisfied: tzdata&gt;=2022.7 in /root/.local/lib/python3.11/site-packages (from pandas-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2025.2)
Collecting annotated-types&gt;=0.6.0 (from pydantic&lt;2.12,&gt;=2.0-&gt;gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.33.2 (from pydantic&lt;2.12,&gt;=2.0-&gt;gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)
Collecting typing-inspection&gt;=0.4.0 (from pydantic&lt;2.12,&gt;=2.0-&gt;gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)
Collecting shellingham&gt;=1.3.0 (from typer&lt;1.0,&gt;=0.12-&gt;gradio-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)
Requirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.17.0)
Collecting contourpy&gt;=1.0.1 (from matplotlib-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)
Collecting cycler&gt;=0.10 (from matplotlib-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading https://download.pytorch.org/whl/test/cycler-0.12.1-py3-none-any.whl (8.3 kB)
Collecting fonttools&gt;=4.22.0 (from matplotlib-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (107 kB)
Collecting kiwisolver&gt;=1.3.1 (from matplotlib-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)
Collecting pyparsing&gt;=2.3.1 (from matplotlib-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)
Collecting distro&lt;2,&gt;=1.7.0 (from openai-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)
Collecting jiter&lt;1,&gt;=0.4.0 (from openai-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)
Collecting texttable (from py7zr-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)
Collecting pycryptodomex&gt;=3.20.0 (from py7zr-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)
Collecting pyzstd&gt;=0.16.1 (from py7zr-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)
Collecting pyppmd&lt;1.3.0,&gt;=1.1.0 (from py7zr-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)
Collecting pybcj&lt;1.1.0,&gt;=1.0.0 (from py7zr-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)
Collecting multivolumefile&gt;=0.2.3 (from py7zr-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)
Collecting inflate64&lt;1.1.0,&gt;=1.0.0 (from py7zr-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
Collecting typing-extensions&gt;=4.8.0 (from llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)
Collecting absl-py (from rouge-score-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)
Collecting nltk (from rouge-score-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)
Collecting joblib (from nltk-&gt;rouge-score-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)
Collecting scikit-learn (from sentence-transformers-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)
Collecting threadpoolctl&gt;=3.1.0 (from scikit-learn-&gt;sentence-transformers-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)
Requirement already satisfied: executing&gt;=1.2.0 in /usr/local/lib/python3.11/site-packages (from stack_data-&gt;ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.2.0)
Requirement already satisfied: asttokens&gt;=2.1.0 in /usr/local/lib/python3.11/site-packages (from stack_data-&gt;ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (3.0.0)
Requirement already satisfied: pure-eval in /usr/local/lib/python3.11/site-packages (from stack_data-&gt;ipython&gt;=7.8.0-&gt;black[jupyter]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.2.3)
Collecting filetype (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)
Collecting python-magic (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)
Collecting lxml (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)
Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/site-packages (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (4.13.4)
Collecting emoji (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)
Collecting dataclasses-json (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)
Collecting python-iso639 (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)
Collecting langdetect (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading langdetect-1.0.9.tar.gz (981 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 190.2 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting rapidfuzz (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)
Collecting backoff (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)
Collecting unstructured-client (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading unstructured_client-0.39.1-py3-none-any.whl.metadata (21 kB)
Collecting python-oxmsg (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)
Collecting html5lib (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)
Collecting onnx&gt;=1.17.0 (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)
Collecting onnxruntime&gt;=1.19.0 (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)
Collecting pdf2image (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)
Collecting pdfminer.six (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)
Collecting pikepdf (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pikepdf-9.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)
Collecting pi-heif (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)
Collecting pypdf (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pypdf-5.8.0-py3-none-any.whl.metadata (7.1 kB)
Collecting google-cloud-vision (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)
Collecting effdet (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)
Collecting unstructured-inference&gt;=1.0.5 (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading unstructured_inference-1.0.5-py3-none-any.whl.metadata (5.3 kB)
Collecting unstructured.pytesseract&gt;=0.3.12 (from unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)
Collecting coloredlogs (from onnxruntime&gt;=1.19.0-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)
Collecting flatbuffers (from onnxruntime&gt;=1.19.0-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)
Collecting opencv-python!=4.7.0.68 (from unstructured-inference&gt;=1.0.5-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)
Collecting timm (from unstructured-inference&gt;=1.0.5-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading timm-1.0.17-py3-none-any.whl.metadata (59 kB)
Collecting pypdfium2 (from unstructured-inference&gt;=1.0.5-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)
Collecting numpy&gt;=1.17 (from transformers!=4.38.*,!=4.39.*)
  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)
Requirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.11/site-packages (from beautifulsoup4-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.7)
Collecting humanfriendly&gt;=9.1 (from coloredlogs-&gt;onnxruntime&gt;=1.19.0-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)
Collecting marshmallow&lt;4.0.0,&gt;=3.18.0 (from dataclasses-json-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)
Collecting typing-inspect&lt;1,&gt;=0.4.0 (from dataclasses-json-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading https://download.pytorch.org/whl/test/typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)
Requirement already satisfied: torchvision in /root/.local/lib/python3.11/site-packages (from effdet-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.22.1)
Collecting pycocotools&gt;=2.0.2 (from effdet-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pycocotools-2.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)
Collecting omegaconf&gt;=2.0 (from effdet-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading https://download.pytorch.org/whl/test/omegaconf-2.3.0-py3-none-any.whl (79 kB)
Collecting antlr4-python3-runtime==4.9.* (from omegaconf&gt;=2.0-&gt;effdet-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading https://download.pytorch.org/whl/test/antlr4_python3_runtime-4.9.3.tar.gz (117 kB)
  Preparing metadata (setup.py) ... done
Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0,&gt;=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0,&gt;=1.34.1-&gt;google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)
Collecting google-auth!=2.24.0,!=2.25.0,&lt;3.0.0,&gt;=2.14.1 (from google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)
Collecting proto-plus&lt;2.0.0,&gt;=1.22.3 (from google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)
Collecting grpcio&lt;2.0.0,&gt;=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0,&gt;=1.34.1-&gt;google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Collecting grpcio-status&lt;2.0.0,&gt;=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0,&gt;=1.34.1-&gt;google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)
Collecting cachetools&lt;6.0,&gt;=2.0.0 (from google-auth!=2.24.0,!=2.25.0,&lt;3.0.0,&gt;=2.14.1-&gt;google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)
Collecting pyasn1-modules&gt;=0.2.1 (from google-auth!=2.24.0,!=2.25.0,&lt;3.0.0,&gt;=2.14.1-&gt;google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)
Collecting rsa&lt;5,&gt;=3.1.4 (from google-auth!=2.24.0,!=2.25.0,&lt;3.0.0,&gt;=2.14.1-&gt;google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)
INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.
Collecting grpcio-status&lt;2.0.0,&gt;=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,&lt;3.0.0,&gt;=1.34.1-&gt;google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)
INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.
  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)
  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)
Collecting pyasn1&gt;=0.1.3 (from rsa&lt;5,&gt;=3.1.4-&gt;google-auth!=2.24.0,!=2.25.0,&lt;3.0.0,&gt;=2.14.1-&gt;google-cloud-vision-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)
Requirement already satisfied: webencodings in /usr/local/lib/python3.11/site-packages (from html5lib-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (0.5.1)
Collecting cryptography&gt;=36.0.0 (from pdfminer.six-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading cryptography-45.0.5-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)
Requirement already satisfied: cffi&gt;=1.14 in /usr/local/lib/python3.11/site-packages (from cryptography&gt;=36.0.0-&gt;pdfminer.six-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.17.1)
Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi&gt;=1.14-&gt;cryptography&gt;=36.0.0-&gt;pdfminer.six-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (2.22)
Collecting olefile (from python-oxmsg-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)
Requirement already satisfied: nest-asyncio&gt;=1.6.0 in /usr/local/lib/python3.11/site-packages (from unstructured-client-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes) (1.6.0)
Collecting requests-toolbelt&gt;=1.0.0 (from unstructured-client-&gt;unstructured[pdf]-&gt;llama-cookbook==0.0.5.post1-&gt;llama-recipes)
  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)
Downloading llama_recipes-0.0.5.post2-py3-none-any.whl (20 kB)
Downloading llama_cookbook-0.0.5.post1-py3-none-any.whl (70 kB)
Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)
Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 186.7 MB/s eta 0:00:00
Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)
Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)
Downloading tokenize_rt-6.2.0-py2.py3-none-any.whl (6.0 kB)
Downloading chardet-5.2.0-py3-none-any.whl (199 kB)
Downloading codeshield-1.0.1-py3-none-any.whl (173 kB)
Downloading semgrep-1.128.1-cp39.cp310.cp311.py39.py310.py311-none-musllinux_1_0_x86_64.manylinux2014_x86_64.whl (48.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.2/48.2 MB 135.2 MB/s eta 0:00:00a 0:00:01
Downloading boltons-21.0.0-py2.py3-none-any.whl (193 kB)
Downloading click-8.1.8-py3-none-any.whl (98 kB)
Downloading click_option_group-0.5.7-py3-none-any.whl (11 kB)
Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)
Downloading glom-22.1.0-py2.py3-none-any.whl (100 kB)
Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)
Downloading opentelemetry_exporter_otlp_proto_http-1.25.0-py3-none-any.whl (16 kB)
Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl (17 kB)
Downloading opentelemetry_proto-1.25.0-py3-none-any.whl (52 kB)
Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)
Downloading opentelemetry_instrumentation_requests-0.46b0-py3-none-any.whl (12 kB)
Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl (29 kB)
Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)
Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl (6.9 kB)
Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)
Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)
Downloading rich-13.5.3-py3-none-any.whl (239 kB)
Downloading tomli-2.0.2-py3-none-any.whl (13 kB)
Downloading wcmatch-8.5.2-py3-none-any.whl (39 kB)
Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)
Downloading bracex-2.6-py3-none-any.whl (11 kB)
Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)
Downloading face-24.0.0-py3-none-any.whl (54 kB)
Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)
Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 739.1/739.1 kB 145.7 MB/s eta 0:00:00
Downloading zipp-3.23.0-py3-none-any.whl (10 kB)
Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)
Downloading gradio-5.38.0-py3-none-any.whl (59.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.6/59.6 MB 139.9 MB/s eta 0:00:00a 0:00:01
Downloading gradio_client-1.11.0-py3-none-any.whl (324 kB)
Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)
Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)
Downloading groovy-0.1.2-py3-none-any.whl (14 kB)
Downloading orjson-3.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (127 kB)
Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)
Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 160.7 MB/s eta 0:00:00
Downloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)
Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)
Downloading starlette-0.47.1-py3-none-any.whl (72 kB)
Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)
Downloading typer-0.16.0-py3-none-any.whl (46 kB)
Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)
Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)
Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 157.9 MB/s eta 0:00:00
Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)
Downloading ruff-0.12.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 140.0 MB/s eta 0:00:00
Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)
Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)
Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)
Downloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)
Downloading loralib-0.1.2-py3-none-any.whl (10 kB)
Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.6/8.6 MB 141.7 MB/s eta 0:00:00
Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)
Downloading fonttools-4.59.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 147.7 MB/s eta 0:00:00
Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 170.7 MB/s eta 0:00:00
Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)
Downloading openai-1.97.0-py3-none-any.whl (764 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 765.0/765.0 kB 190.8 MB/s eta 0:00:00
Downloading distro-1.9.0-py3-none-any.whl (20 kB)
Downloading jiter-0.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)
Downloading optimum-1.26.1-py3-none-any.whl (424 kB)
Downloading py7zr-1.0.0-py3-none-any.whl (69 kB)
Downloading inflate64-1.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (96 kB)
Downloading pybcj-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)
Downloading pyppmd-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)
Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)
Downloading pycryptodomex-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 173.5 MB/s eta 0:00:00
Downloading pyzstd-0.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)
Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)
Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)
Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)
Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 168.6 MB/s eta 0:00:00
Downloading joblib-1.5.1-py3-none-any.whl (307 kB)
Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.3/35.3 MB 154.0 MB/s eta 0:00:00a 0:00:01
Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)
Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.7/9.7 MB 149.0 MB/s eta 0:00:00
Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)
Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)
Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)
Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)
Downloading unstructured-0.18.9-py3-none-any.whl (1.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 163.8 MB/s eta 0:00:00
Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.6/17.6 MB 153.1 MB/s eta 0:00:00
Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 151.8 MB/s eta 0:00:00
Downloading unstructured_inference-1.0.5-py3-none-any.whl (48 kB)
Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 MB 149.8 MB/s eta 0:00:00a 0:00:01
Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 154.4 MB/s eta 0:00:00
Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)
Downloading backoff-2.2.1-py3-none-any.whl (15 kB)
Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)
Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)
Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)
Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)
Downloading https://download.pytorch.org/whl/test/typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)
Downloading effdet-0.4.1-py3-none-any.whl (112 kB)
Downloading pycocotools-2.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (477 kB)
Downloading timm-1.0.17-py3-none-any.whl (2.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 163.9 MB/s eta 0:00:00
Downloading emoji-2.14.1-py3-none-any.whl (590 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 590.6/590.6 kB 191.4 MB/s eta 0:00:00
Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)
Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)
Downloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 527.9/527.9 kB 184.8 MB/s eta 0:00:00
Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)
Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)
Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)
Downloading grpcio-1.73.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 144.1 MB/s eta 0:00:00
Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)
Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)
Downloading rsa-4.9.1-py3-none-any.whl (34 kB)
Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)
Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)
Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)
Downloading lxml-6.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 149.0 MB/s eta 0:00:00
Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)
Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 150.0 MB/s eta 0:00:00
Downloading cryptography-45.0.5-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 165.9 MB/s eta 0:00:00
Downloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 197.5 MB/s eta 0:00:00
Downloading pikepdf-9.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 144.1 MB/s eta 0:00:00
Downloading pypdf-5.8.0-py3-none-any.whl (309 kB)
Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 140.6 MB/s eta 0:00:00
Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)
Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)
Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)
Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)
Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 154.2 MB/s eta 0:00:00
Downloading unstructured_client-0.39.1-py3-none-any.whl (212 kB)
Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)
Building wheels for collected packages: markupsafe, peewee, fire, rouge-score, antlr4-python3-runtime, langdetect
  DEPRECATION: Building 'markupsafe' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'markupsafe'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for markupsafe (setup.py) ... done
  Created wheel for markupsafe: filename=MarkupSafe-2.0.1-py3-none-any.whl size=9745 sha256=7e2d66f9e7f03fb5c9650b1bb42b497988d9caf286498a83e97842c9bd37bfd8
  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/ea/18/79/6266ea508b8164a77b95aa19534c77eb805f2878612c37efca
  Building wheel for peewee (pyproject.toml) ... done
  Created wheel for peewee: filename=peewee-3.18.2-py3-none-any.whl size=139106 sha256=74775c98fa5491eac6deed3b5a8d8c2e44da24939d4780973eaec27fdde604ca
  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/28/84/61/758d1bd7b9c9d700158c8642a8aff2a9bf2e1ae69641c40784
  DEPRECATION: Building 'fire' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'fire'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for fire (setup.py) ... done
  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=604933afdfa2c129d2a0233ffaccf9ce24822ded3a417ab33a6f781ddc81d7af
  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89
  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for rouge-score (setup.py) ... done
  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=0831f9eb0a69648a284b2f8bc386bdee2e9c262f67eb5d95bd6bffb09eaec1fc
  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148
  DEPRECATION: Building 'antlr4-python3-runtime' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'antlr4-python3-runtime'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for antlr4-python3-runtime (setup.py) ... done
  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=187021b4cd7030f0ba2c29c20fdd1655628f2e6f082d8f5ae91f16dd343995bd
  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/56/e9/6d/b5ab1c9ab438ad8897f796286bf23cd4ffc0f1ea8bc2200ecd
  DEPRECATION: Building 'langdetect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'langdetect'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for langdetect (setup.py) ... done
  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=a906ce17a25a949ae03647d998c1541f9c12b603f2c439fdef6983b2076421fc
  Stored in directory: /tmp/pip-ephem-wheel-cache-ysln68d0/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e
Successfully built markupsafe peewee fire rouge-score antlr4-python3-runtime langdetect
Installing collected packages: texttable, sentencepiece, pydub, peewee, flatbuffers, filetype, brotli, boltons, appdirs, antlr4-python3-runtime, zipp, wrapt, websockets, unstructured.pytesseract, typing-extensions, tomlkit, tomli, tokenize-rt, threadpoolctl, termcolor, tabulate, shellingham, semantic-version, ruff, ruamel.yaml.clib, rapidfuzz, pyyaml, python-multipart, python-magic, python-iso639, pyppmd, pypdfium2, pypdf, pyparsing, pycryptodomex, pybcj, pyasn1, protobuf, pi-heif, pdf2image, pathspec, orjson, opentelemetry-util-http, olefile, numpy, mypy-extensions, multivolumefile, mdurl, marshmallow, markupsafe, lxml, loralib, langdetect, kiwisolver, joblib, jiter, inflate64, humanfriendly, html5lib, grpcio, groovy, fonttools, ffmpy, face, exceptiongroup, emoji, distro, cycler, colorama, click, chardet, cachetools, bracex, backoff, annotated-types, aiofiles, absl-py, wcmatch, uvicorn, typing-inspection, typing-inspect, scipy, ruamel.yaml, rsa, requests-toolbelt, pyzstd, python-oxmsg, pydantic-core, pycocotools, pyasn1-modules, proto-plus, opentelemetry-proto, opencv-python, onnx, omegaconf, nltk, markdown-it-py, importlib-metadata, googleapis-common-protos, glom, fire, deprecated, cryptography, contourpy, coloredlogs, click-option-group, black, starlette, scikit-learn, rouge-score, rich, pydantic, py7zr, pikepdf, pdfminer.six, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, matplotlib, grpcio-status, google-auth, dataclasses-json, unstructured-client, typer, safehttpx, opentelemetry-semantic-conventions, opentelemetry-instrumentation, openai, gradio-client, google-api-core, fastapi, unstructured, timm, sentence-transformers, optimum, opentelemetry-sdk, opentelemetry-instrumentation-requests, gradio, evaluate, unstructured-inference, opentelemetry-exporter-otlp-proto-http, google-cloud-vision, effdet, semgrep, codeshield, llama-cookbook, llama-recipes
  Attempting uninstall: typing-extensions━━━━━━━━━━━━━━━━━━━━━━━━━  12/147 [websockets]on3-runtime]
    Found existing installation: typing_extensions 4.12.2━━━━━  12/147 [websockets]
    Uninstalling typing_extensions-4.12.2:━━━━━━━━━━━━━━━━━━━━  12/147 [websockets]
      Successfully uninstalled typing_extensions-4.12.2━━━━━━━━━━━  14/147 [typing-extensions]
  Attempting uninstall: pyyaml90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  25/147 [rapidfuzz]ons]
    Found existing installation: PyYAML 6.0.2━━━━━━━━━━━━━━━━━  25/147 [rapidfuzz]
    Uninstalling PyYAML-6.0.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  25/147 [rapidfuzz]
      Successfully uninstalled PyYAML-6.0.2━━━━━━━━━━━━━━━━━━━  25/147 [rapidfuzz]
  Attempting uninstall: protobuf[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  36/147 [pyasn1]odomex]
    Found existing installation: protobuf 5.29.5━━━━━━━━━━━━━━  36/147 [pyasn1]
    Uninstalling protobuf-5.29.5:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  36/147 [pyasn1]
      Successfully uninstalled protobuf-5.29.5━━━━━━━━━━━━━━━━━━━━  37/147 [protobuf]
  Attempting uninstall: numpy[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  37/147 [protobuf]
    Found existing installation: numpy 2.3.1━━━━━━━━━━━━━━━━━━━━━━  44/147 [numpy]
    Uninstalling numpy-2.3.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━  44/147 [numpy]
      Successfully uninstalled numpy-2.3.1━━━━━━━━━━━━━━━━━━━━  44/147 [numpy]
  Attempting uninstall: markupsafe[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━  44/147 [numpy]
    Found existing installation: MarkupSafe 3.0.2━━━━━━━━━━━━━  44/147 [numpy]
    Uninstalling MarkupSafe-3.0.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━  44/147 [numpy]
      Successfully uninstalled MarkupSafe-3.0.2━━━━━━━━━━━━━━━━━━━  49/147 [markupsafe]
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147/147 [llama-recipes]hield]mgrep]loud-vision]ce]ons]
Successfully installed absl-py-2.3.1 aiofiles-24.1.0 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 backoff-2.2.1 black-25.1.0 boltons-21.0.0 bracex-2.6 brotli-1.1.0 cachetools-5.5.2 chardet-5.2.0 click-8.1.8 click-option-group-0.5.7 codeshield-1.0.1 colorama-0.4.6 coloredlogs-15.0.1 contourpy-1.3.2 cryptography-45.0.5 cycler-0.12.1 dataclasses-json-0.6.7 deprecated-1.2.18 distro-1.9.0 effdet-0.4.1 emoji-2.14.1 evaluate-0.4.5 exceptiongroup-1.2.2 face-24.0.0 fastapi-0.116.1 ffmpy-0.6.0 filetype-1.2.0 fire-0.7.0 flatbuffers-25.2.10 fonttools-4.59.0 glom-22.1.0 google-api-core-2.25.1 google-auth-2.40.3 google-cloud-vision-3.10.2 googleapis-common-protos-1.70.0 gradio-5.38.0 gradio-client-1.11.0 groovy-0.1.2 grpcio-1.73.1 grpcio-status-1.62.3 html5lib-1.1 humanfriendly-10.0 importlib-metadata-7.1.0 inflate64-1.0.3 jiter-0.10.0 joblib-1.5.1 kiwisolver-1.4.8 langdetect-1.0.9 llama-cookbook-0.0.5.post1 llama-recipes-0.0.5.post2 loralib-0.1.2 lxml-6.0.0 markdown-it-py-3.0.0 markupsafe-2.0.1 marshmallow-3.26.1 matplotlib-3.10.3 mdurl-0.1.2 multivolumefile-0.2.3 mypy-extensions-1.1.0 nltk-3.9.1 numpy-2.2.6 olefile-0.47 omegaconf-2.3.0 onnx-1.18.0 onnxruntime-1.22.1 openai-1.97.0 opencv-python-4.12.0.88 opentelemetry-api-1.25.0 opentelemetry-exporter-otlp-proto-common-1.25.0 opentelemetry-exporter-otlp-proto-http-1.25.0 opentelemetry-instrumentation-0.46b0 opentelemetry-instrumentation-requests-0.46b0 opentelemetry-proto-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 opentelemetry-util-http-0.46b0 optimum-1.26.1 orjson-3.11.0 pathspec-0.12.1 pdf2image-1.17.0 pdfminer.six-20250506 peewee-3.18.2 pi-heif-1.0.0 pikepdf-9.10.2 proto-plus-1.26.1 protobuf-4.25.8 py7zr-1.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pybcj-1.0.6 pycocotools-2.0.10 pycryptodomex-3.23.0 pydantic-2.11.7 pydantic-core-2.33.2 pydub-0.25.1 pyparsing-3.2.3 pypdf-5.8.0 pypdfium2-4.30.1 pyppmd-1.2.0 python-iso639-2025.2.18 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.2 pyyaml-6.0.1 pyzstd-0.17.0 rapidfuzz-3.13.0 requests-toolbelt-1.0.0 rich-13.5.3 rouge-score-0.1.2 rsa-4.9.1 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 ruff-0.12.4 safehttpx-0.1.6 scikit-learn-1.7.1 scipy-1.16.0 semantic-version-2.10.0 semgrep-1.128.1 sentence-transformers-5.0.0 sentencepiece-0.2.0 shellingham-1.5.4 starlette-0.47.1 tabulate-0.9.0 termcolor-3.1.0 texttable-1.7.0 threadpoolctl-3.6.0 timm-1.0.17 tokenize-rt-6.2.0 tomli-2.0.2 tomlkit-0.13.3 typer-0.16.0 typing-extensions-4.14.1 typing-inspect-0.9.0 typing-inspection-0.4.1 unstructured-0.18.9 unstructured-client-0.39.1 unstructured-inference-1.0.5 unstructured.pytesseract-0.3.15 uvicorn-0.35.0 wcmatch-8.5.2 websockets-15.0.1 wrapt-1.17.2 zipp-3.23.0
Note: you may need to restart the kernel to use updated packages.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install bitsandbytes<span class="op">&gt;=</span><span class="fl">0.43.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Note: you may need to restart the kernel to use updated packages.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> login</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> getpass</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get your token securely</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>hf_token <span class="op">=</span> getpass.getpass(<span class="st">"Enter your Hugging Face token: "</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Login programmatically</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>login(token<span class="op">=</span>hf_token)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"✅ Successfully logged in to Hugging Face!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/root/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm</code></pre>
</div>
<div class="cell-output cell-output-stdin">
<pre><code>Enter your Hugging Face token:  ········</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>✅ Successfully logged in to Hugging Face!</code></pre>
</div>
</div>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> subprocess</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set environment variable</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'BNB_CUDA_VERSION'</span>] <span class="op">=</span> <span class="st">'125'</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Install ONLY the essential fixes</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>commands <span class="op">=</span> [</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"pip"</span>, <span class="st">"install"</span>, <span class="st">"transformers==4.47.1"</span>, <span class="st">"--upgrade"</span>],</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"pip"</span>, <span class="st">"install"</span>, <span class="st">"bitsandbytes&gt;=0.43.0"</span>, <span class="st">"--upgrade"</span>, <span class="st">"--force-reinstall"</span>]</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cmd <span class="kw">in</span> commands:</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Running: </span><span class="sc">{</span><span class="st">' '</span><span class="sc">.</span>join(cmd)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> subprocess.run(cmd, capture_output<span class="op">=</span><span class="va">True</span>, text<span class="op">=</span><span class="va">True</span>, timeout<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> result.returncode <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Error: </span><span class="sc">{</span>result<span class="sc">.</span>stderr<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"✅ Success"</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> subprocess.TimeoutExpired:</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"⚠️ Command timed out"</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"✅ Essential dependencies updated"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Running: pip install transformers==4.47.1 --upgrade
✅ Success
Running: pip install bitsandbytes&gt;=0.43.0 --upgrade --force-reinstall
✅ Success
✅ Essential dependencies updated</code></pre>
</div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>os.chdir(<span class="st">"fsdp_qlora"</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply fixes to train.py</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"train.py"</span>, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    content <span class="op">=</span> f.read()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply transformers fix</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="st">"LLAMA_ATTENTION_CLASSES"</span> <span class="kw">in</span> content:</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"🔧 Applying transformers fix..."</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simple replacement approach</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    content <span class="op">=</span> content.replace(</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LLAMA_ATTENTION_CLASSES,"</span>, </span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">"LlamaAttention,"</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    content <span class="op">=</span> content.replace(</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="st">"MISTRAL_ATTENTION_CLASSES,"</span>, </span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        <span class="st">"MistralAttention,"</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    content <span class="op">=</span> content.replace(</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        <span class="st">"(*LLAMA_ATTENTION_CLASSES.values(), *MISTRAL_ATTENTION_CLASSES.values())"</span>,</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>        <span class="st">"(LlamaAttention, MistralAttention)"</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add dataset choice</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"uganda_clinical_guidelines"</span> <span class="kw">not</span> <span class="kw">in</span> content:</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>        content <span class="op">=</span> content.replace(</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>            <span class="st">'"orca_math"]) = "alpaca_sample",'</span>,</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>            <span class="st">'"orca_math", "uganda_clinical_guidelines"]) = "alpaca_sample",'</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">"train.py"</span>, <span class="st">"w"</span>) <span class="im">as</span> f:</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>        f.write(content)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✅ train.py fixed"</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>🔧 Applying transformers fix...
✅ train.py fixed</code></pre>
</div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test if fixes work</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> bitsandbytes</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"✅ Bitsandbytes works"</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"❌ Bitsandbytes issue: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ready for training!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>❌ Bitsandbytes issue: Failed to find C compiler. Please specify via CC environment variable.
Ready for training!</code></pre>
</div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="6">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> subprocess</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup_environment():</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Setup the environment to avoid compiler issues"""</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"🔧 Setting up environment for training..."</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 1: Set environment variables</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">'BNB_CUDA_VERSION'</span>] <span class="op">=</span> <span class="st">'125'</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">'CC'</span>] <span class="op">=</span> <span class="st">'/usr/bin/gcc'</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">'CXX'</span>] <span class="op">=</span> <span class="st">'/usr/bin/g++'</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    os.environ[<span class="st">'CUDA_VISIBLE_DEVICES'</span>] <span class="op">=</span> <span class="st">'0,1'</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 2: Install build tools if possible</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"📦 Installing build tools..."</span>)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>        subprocess.run([<span class="st">"apt"</span>, <span class="st">"update"</span>], capture_output<span class="op">=</span><span class="va">True</span>, timeout<span class="op">=</span><span class="dv">60</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> subprocess.run([<span class="st">"apt"</span>, <span class="st">"install"</span>, <span class="st">"-y"</span>, <span class="st">"build-essential"</span>, <span class="st">"gcc"</span>, <span class="st">"g++"</span>], </span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>                              capture_output<span class="op">=</span><span class="va">True</span>, timeout<span class="op">=</span><span class="dv">120</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> result.returncode <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"✅ Build tools installed"</span>)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"⚠️ Build tools installation failed, proceeding anyway..."</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"⚠️ Could not install build tools: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Step 3: Test if bitsandbytes works now</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>        <span class="im">import</span> bitsandbytes</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"✅ Bitsandbytes imports successfully"</span>)</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"❌ Bitsandbytes still has issues: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Step 4: Try installing older version</span></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"🔄 Trying older bitsandbytes version..."</span>)</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>            subprocess.run([<span class="st">"pip"</span>, <span class="st">"uninstall"</span>, <span class="st">"bitsandbytes"</span>, <span class="st">"-y"</span>], capture_output<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>            subprocess.run([<span class="st">"pip"</span>, <span class="st">"install"</span>, <span class="st">"bitsandbytes==0.41.3"</span>], capture_output<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>            <span class="im">import</span> bitsandbytes</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"✅ Older bitsandbytes version works"</span>)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">True</span></span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e2:</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"❌ Even older version failed: </span><span class="sc">{</span>e2<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">False</span></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the setup</span></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> setup_environment():</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"🚀 Environment ready! Running training..."</span>)</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Your training command</span></span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>    cmd <span class="op">=</span> [</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>        <span class="st">"python"</span>, <span class="st">"train.py"</span>,</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--train_type"</span>, <span class="st">"bnb_dora"</span>,</span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--model_name"</span>, <span class="st">"meta-llama/Llama-2-7b-hf"</span>, </span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--dataset"</span>, <span class="st">"ug_clinical_guidelines"</span>,  <span class="co"># Fixed dataset name</span></span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--dataset_samples"</span>, <span class="st">"10"</span>,</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--batch_size"</span>, <span class="st">"1"</span>,</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--context_length"</span>, <span class="st">"256"</span>,</span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--num_epochs"</span>, <span class="st">"1"</span>,</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--save_model"</span>, <span class="st">"false"</span>,</span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>        <span class="st">"--log_to"</span>, <span class="st">"stdout"</span></span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"🧪 Running test training..."</span>)</span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true" tabindex="-1"></a>    process <span class="op">=</span> subprocess.Popen(cmd, stdout<span class="op">=</span>subprocess.PIPE, stderr<span class="op">=</span>subprocess.STDOUT, text<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb21-71"><a href="#cb21-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> <span class="bu">iter</span>(process.stdout.readline, <span class="st">''</span>):</span>
<span id="cb21-72"><a href="#cb21-72" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> line:</span>
<span id="cb21-73"><a href="#cb21-73" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(line.rstrip())</span>
<span id="cb21-74"><a href="#cb21-74" aria-hidden="true" tabindex="-1"></a>        process.wait()</span>
<span id="cb21-75"><a href="#cb21-75" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Test completed: </span><span class="sc">{</span>process<span class="sc">.</span>returncode<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-76"><a href="#cb21-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">KeyboardInterrupt</span>:</span>
<span id="cb21-77"><a href="#cb21-77" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Interrupted"</span>)</span>
<span id="cb21-78"><a href="#cb21-78" aria-hidden="true" tabindex="-1"></a>        process.terminate()</span>
<span id="cb21-79"><a href="#cb21-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-80"><a href="#cb21-80" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb21-81"><a href="#cb21-81" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"❌ Could not setup environment properly"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>🔧 Setting up environment for training...
📦 Installing build tools...
✅ Build tools installed
✅ Bitsandbytes imports successfully
🚀 Environment ready! Running training...
🧪 Running test training...
WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

World size: 2
WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=


Generating train split:   0%|          | 0/130 [00:00&lt;?, ? examples/s]
Generating train split: 100%|██████████| 130/130 [00:00&lt;00:00, 23549.26 examples/s]
Creating model 0

Downloading shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]
Downloading shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]
Downloading shards:  50%|█████     | 1/2 [00:48&lt;00:48, 48.18s/it]
Downloading shards:  50%|█████     | 1/2 [00:48&lt;00:48, 48.21s/it]
Downloading shards: 100%|██████████| 2/2 [01:04&lt;00:00, 29.56s/it]
Downloading shards: 100%|██████████| 2/2 [01:04&lt;00:00, 32.35s/it]
Loading model 0

Loading &amp; Quantizing Model Shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]
Downloading shards: 100%|██████████| 2/2 [01:04&lt;00:00, 29.57s/it]
Downloading shards: 100%|██████████| 2/2 [01:04&lt;00:00, 32.36s/it]
WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=


Loading &amp; Quantizing Model Shards:  50%|█████     | 1/2 [00:15&lt;00:15, 15.38s/it]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=


Loading &amp; Quantizing Model Shards: 100%|██████████| 2/2 [00:25&lt;00:00, 12.20s/it]
Loading &amp; Quantizing Model Shards: 100%|██████████| 2/2 [00:25&lt;00:00, 12.68s/it]
/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Rank 0: Model created: 0.107 GiB
Using BNB DORA 0
Rank 0: LoRA layers added: 0.107 GiB
Wrapping model w/ FSDP 0
Rank 0: Wrapped model: 1.625 GiB
Applying activation checkpointing 0
Total Training Steps: 5

  0%|          | 0/5 [00:00&lt;?, ?it/s]
Epoch 0, Loss 0.000:   0%|          | 0/5 [00:00&lt;?, ?it/s]
Epoch 0, Loss 0.000:  20%|██        | 1/5 [00:07&lt;00:28,  7.01s/it]
Epoch 0, Loss 1.388, LR 1.00e-05:  20%|██        | 1/5 [00:07&lt;00:28,  7.01s/it]
Epoch 0, Loss 1.388, LR 1.00e-05:  40%|████      | 2/5 [00:09&lt;00:12,  4.30s/it]
Epoch 0, Loss 1.477, LR 1.00e-05:  40%|████      | 2/5 [00:09&lt;00:12,  4.30s/it]
Epoch 0, Loss 1.477, LR 1.00e-05:  60%|██████    | 3/5 [00:11&lt;00:06,  3.23s/it]
Epoch 0, Loss 1.187, LR 1.00e-05:  60%|██████    | 3/5 [00:11&lt;00:06,  3.23s/it]
Epoch 0, Loss 1.187, LR 1.00e-05:  80%|████████  | 4/5 [00:13&lt;00:02,  2.71s/it]
Epoch 0, Loss 1.041, LR 1.00e-05:  80%|████████  | 4/5 [00:13&lt;00:02,  2.71s/it]
Epoch 0, Loss 1.041, LR 1.00e-05: 100%|██████████| 5/5 [00:15&lt;00:00,  2.43s/it]
Epoch 0, Loss 1.475, LR 1.00e-05: 100%|██████████| 5/5 [00:15&lt;00:00,  2.43s/it]
Epoch 0, Loss 1.475, LR 1.00e-05: 100%|██████████| 5/5 [00:15&lt;00:00,  3.12s/it]
Finished training 0
CUDA event elapsed time: 15.2380859375 sec
time_taken: 15.2380859375
Rank 0: Before forward: 1.62 GiB
Rank 0: After forward: 2.46 GiB
Rank 0: After backward: 2.64 GiB
Rank 0: Peak allocated memory: 1.35 GiB
Rank 0: Peak reserved memory:  2.65 GiB
Using BNB DORA 1
Test completed: 0</code></pre>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>ls</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>'Converting the State Dict.ipynb'   fsdp_multi_node.sh   tests
 LICENSE                hf_train.py      train.py
 PROFILING.md               nbs          train.sh
 README.md              profile.sh       train_hqq_bench.sh
 __pycache__                profiling_utils.py   train_sql.sh
 benchmarking               scripts
 benchmarks_03_2024.md          table1.sh</code></pre>
</div>
</div>
<div class="cell" data-scrolled="true" data-execution_count="9">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> subprocess</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set environment</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'BNB_CUDA_VERSION'</span>] <span class="op">=</span> <span class="st">'125'</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">'CUDA_VISIBLE_DEVICES'</span>] <span class="op">=</span> <span class="st">'0,1'</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># FULL TRAINING with model saving</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>cmd <span class="op">=</span> [</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"python"</span>, <span class="st">"train.py"</span>,</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--train_type"</span>, <span class="st">"bnb_dora"</span>,</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--model_name"</span>, <span class="st">"meta-llama/Llama-2-7b-hf"</span>, </span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--dataset"</span>, <span class="st">"ug_clinical_guidelines"</span>,</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--dataset_samples"</span>, <span class="st">"130"</span>,  <span class="co"># Use all your data</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--batch_size"</span>, <span class="st">"2"</span>,</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--context_length"</span>, <span class="st">"512"</span>,   <span class="co"># Longer context for medical text</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--precision"</span>, <span class="st">"bf16"</span>,</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--num_epochs"</span>, <span class="st">"3"</span>,         <span class="co"># More epochs for better training</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--save_model"</span>, <span class="st">"true"</span>,      <span class="co"># 🔥 SAVE THE MODEL</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--output_dir"</span>, <span class="st">"./uganda_clinical_model"</span>,  <span class="co"># Where to save</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    <span class="st">"--log_to"</span>, <span class="st">"stdout"</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"🏥 Training Uganda Clinical Model (FULL)..."</span>)</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>process <span class="op">=</span> subprocess.Popen(cmd, stdout<span class="op">=</span>subprocess.PIPE, stderr<span class="op">=</span>subprocess.STDOUT, text<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> <span class="bu">iter</span>(process.stdout.readline, <span class="st">''</span>):</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> line:</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(line.rstrip())</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>    process.wait()</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Training completed: </span><span class="sc">{</span>process<span class="sc">.</span>returncode<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check if model was saved</span></span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.path.exists(<span class="st">"uganda_clinical_model"</span>):</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"🎉 Model saved successfully!"</span>)</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"📁 Saved files:"</span>)</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> f <span class="kw">in</span> os.listdir(<span class="st">"uganda_clinical_model"</span>):</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  📄 </span><span class="sc">{</span>f<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">KeyboardInterrupt</span>:</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Interrupted"</span>)</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>    process.terminate()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>🏥 Training Uganda Clinical Model (FULL)...
WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

World size: 2
WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

Creating model 0
Loading model 0

Loading &amp; Quantizing Model Shards:   0%|          | 0/2 [00:00&lt;?, ?it/s]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=

WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=


Loading &amp; Quantizing Model Shards:  50%|█████     | 1/2 [00:15&lt;00:15, 15.16s/it]WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.
This can be used to load a bitsandbytes version built with a CUDA version that is different from the PyTorch CUDA version.
If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=


Loading &amp; Quantizing Model Shards: 100%|██████████| 2/2 [00:26&lt;00:00, 12.62s/it]
Loading &amp; Quantizing Model Shards: 100%|██████████| 2/2 [00:26&lt;00:00, 13.00s/it]
/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user.
  warnings.warn(  # warn only once
Rank 0: Model created: 0.107 GiB
Using BNB DORA 0
Rank 0: LoRA layers added: 0.107 GiB
Wrapping model w/ FSDP 0
Rank 0: Wrapped model: 1.625 GiB
Applying activation checkpointing 0
Total Training Steps: 99

  0%|          | 0/99 [00:00&lt;?, ?it/s]
Epoch 0, Loss 0.000:   0%|          | 0/99 [00:00&lt;?, ?it/s]
Epoch 0, Loss 0.000:   1%|          | 1/99 [00:08&lt;13:29,  8.26s/it]
Epoch 0, Loss 1.426, LR 1.00e-05:   1%|          | 1/99 [00:08&lt;13:29,  8.26s/it]
Epoch 0, Loss 1.426, LR 1.00e-05:   2%|▏         | 2/99 [00:10&lt;07:31,  4.66s/it]
Epoch 0, Loss 1.409, LR 1.00e-05:   2%|▏         | 2/99 [00:10&lt;07:31,  4.66s/it]
Epoch 0, Loss 1.409, LR 1.00e-05:   3%|▎         | 3/99 [00:12&lt;05:27,  3.41s/it]
Epoch 0, Loss 1.369, LR 1.00e-05:   3%|▎         | 3/99 [00:12&lt;05:27,  3.41s/it]
Epoch 0, Loss 1.369, LR 1.00e-05:   4%|▍         | 4/99 [00:14&lt;04:27,  2.82s/it]
Epoch 0, Loss 1.200, LR 1.00e-05:   4%|▍         | 4/99 [00:14&lt;04:27,  2.82s/it]
Epoch 0, Loss 1.200, LR 1.00e-05:   5%|▌         | 5/99 [00:16&lt;03:55,  2.51s/it]
Epoch 0, Loss 1.123, LR 1.00e-05:   5%|▌         | 5/99 [00:16&lt;03:55,  2.51s/it]
Epoch 0, Loss 1.123, LR 1.00e-05:   6%|▌         | 6/99 [00:18&lt;03:37,  2.34s/it]
Epoch 0, Loss 1.126, LR 1.00e-05:   6%|▌         | 6/99 [00:18&lt;03:37,  2.34s/it]
Epoch 0, Loss 1.126, LR 1.00e-05:   7%|▋         | 7/99 [00:20&lt;03:23,  2.21s/it]
Epoch 0, Loss 0.955, LR 1.00e-05:   7%|▋         | 7/99 [00:20&lt;03:23,  2.21s/it]
Epoch 0, Loss 0.955, LR 1.00e-05:   8%|▊         | 8/99 [00:22&lt;03:20,  2.21s/it]
Epoch 0, Loss 0.910, LR 1.00e-05:   8%|▊         | 8/99 [00:22&lt;03:20,  2.21s/it]
Epoch 0, Loss 0.910, LR 1.00e-05:   9%|▉         | 9/99 [00:24&lt;03:12,  2.14s/it]
Epoch 0, Loss 0.948, LR 1.00e-05:   9%|▉         | 9/99 [00:24&lt;03:12,  2.14s/it]
Epoch 0, Loss 0.948, LR 1.00e-05:  10%|█         | 10/99 [00:26&lt;03:05,  2.08s/it]
Epoch 0, Loss 0.674, LR 1.00e-05:  10%|█         | 10/99 [00:26&lt;03:05,  2.08s/it]
Epoch 0, Loss 0.674, LR 1.00e-05:  11%|█         | 11/99 [00:28&lt;02:58,  2.03s/it]
Epoch 0, Loss 0.956, LR 1.00e-05:  11%|█         | 11/99 [00:28&lt;02:58,  2.03s/it]
Epoch 0, Loss 0.956, LR 1.00e-05:  12%|█▏        | 12/99 [00:30&lt;02:54,  2.01s/it]
Epoch 0, Loss 0.994, LR 1.00e-05:  12%|█▏        | 12/99 [00:30&lt;02:54,  2.01s/it]
Epoch 0, Loss 0.994, LR 1.00e-05:  13%|█▎        | 13/99 [00:32&lt;02:52,  2.00s/it]
Epoch 0, Loss 0.803, LR 1.00e-05:  13%|█▎        | 13/99 [00:32&lt;02:52,  2.00s/it]
Epoch 0, Loss 0.803, LR 1.00e-05:  14%|█▍        | 14/99 [00:34&lt;02:49,  2.00s/it]
Epoch 0, Loss 0.902, LR 1.00e-05:  14%|█▍        | 14/99 [00:34&lt;02:49,  2.00s/it]
Epoch 0, Loss 0.902, LR 1.00e-05:  15%|█▌        | 15/99 [00:36&lt;02:53,  2.07s/it]
Epoch 0, Loss 1.091, LR 1.00e-05:  15%|█▌        | 15/99 [00:36&lt;02:53,  2.07s/it]
Epoch 0, Loss 1.091, LR 1.00e-05:  16%|█▌        | 16/99 [00:38&lt;02:49,  2.04s/it]
Epoch 0, Loss 0.834, LR 1.00e-05:  16%|█▌        | 16/99 [00:38&lt;02:49,  2.04s/it]
Epoch 0, Loss 0.834, LR 1.00e-05:  17%|█▋        | 17/99 [00:40&lt;02:44,  2.00s/it]
Epoch 0, Loss 1.042, LR 1.00e-05:  17%|█▋        | 17/99 [00:40&lt;02:44,  2.00s/it]
Epoch 0, Loss 1.042, LR 1.00e-05:  18%|█▊        | 18/99 [00:42&lt;02:39,  1.97s/it]
Epoch 0, Loss 0.731, LR 1.00e-05:  18%|█▊        | 18/99 [00:42&lt;02:39,  1.97s/it]
Epoch 0, Loss 0.731, LR 1.00e-05:  19%|█▉        | 19/99 [00:44&lt;02:35,  1.95s/it]
Epoch 0, Loss 1.042, LR 1.00e-05:  19%|█▉        | 19/99 [00:44&lt;02:35,  1.95s/it]
Epoch 0, Loss 1.042, LR 1.00e-05:  20%|██        | 20/99 [00:45&lt;02:33,  1.94s/it]
Epoch 0, Loss 1.062, LR 1.00e-05:  20%|██        | 20/99 [00:46&lt;02:33,  1.94s/it]
Epoch 0, Loss 1.062, LR 1.00e-05:  21%|██        | 21/99 [00:47&lt;02:31,  1.95s/it]
Epoch 0, Loss 0.817, LR 1.00e-05:  21%|██        | 21/99 [00:47&lt;02:31,  1.95s/it]
Epoch 0, Loss 0.817, LR 1.00e-05:  22%|██▏       | 22/99 [00:50&lt;02:36,  2.04s/it]
Epoch 0, Loss 0.928, LR 1.00e-05:  22%|██▏       | 22/99 [00:50&lt;02:36,  2.04s/it]
Epoch 0, Loss 0.928, LR 1.00e-05:  23%|██▎       | 23/99 [00:52&lt;02:32,  2.01s/it]
Epoch 0, Loss 1.187, LR 1.00e-05:  23%|██▎       | 23/99 [00:52&lt;02:32,  2.01s/it]
Epoch 0, Loss 1.187, LR 1.00e-05:  24%|██▍       | 24/99 [00:54&lt;02:29,  1.99s/it]
Epoch 0, Loss 1.039, LR 1.00e-05:  24%|██▍       | 24/99 [00:54&lt;02:29,  1.99s/it]
Epoch 0, Loss 1.039, LR 1.00e-05:  25%|██▌       | 25/99 [00:56&lt;02:27,  1.99s/it]
Epoch 0, Loss 0.800, LR 1.00e-05:  25%|██▌       | 25/99 [00:56&lt;02:27,  1.99s/it]
Epoch 0, Loss 0.800, LR 1.00e-05:  26%|██▋       | 26/99 [00:58&lt;02:24,  1.98s/it]
Epoch 0, Loss 0.946, LR 1.00e-05:  26%|██▋       | 26/99 [00:58&lt;02:24,  1.98s/it]
Epoch 0, Loss 0.946, LR 1.00e-05:  27%|██▋       | 27/99 [01:00&lt;02:23,  1.99s/it]
Epoch 0, Loss 1.006, LR 1.00e-05:  27%|██▋       | 27/99 [01:00&lt;02:23,  1.99s/it]
Epoch 0, Loss 1.006, LR 1.00e-05:  28%|██▊       | 28/99 [01:01&lt;02:20,  1.98s/it]
Epoch 0, Loss 0.677, LR 1.00e-05:  28%|██▊       | 28/99 [01:01&lt;02:20,  1.98s/it]
Epoch 0, Loss 0.677, LR 1.00e-05:  29%|██▉       | 29/99 [01:04&lt;02:22,  2.04s/it]
Epoch 0, Loss 1.013, LR 1.00e-05:  29%|██▉       | 29/99 [01:04&lt;02:22,  2.04s/it]
Epoch 0, Loss 1.013, LR 1.00e-05:  30%|███       | 30/99 [01:06&lt;02:18,  2.01s/it]
Epoch 0, Loss 0.918, LR 1.00e-05:  30%|███       | 30/99 [01:06&lt;02:18,  2.01s/it]
Epoch 0, Loss 0.918, LR 1.00e-05:  31%|███▏      | 31/99 [01:08&lt;02:16,  2.01s/it]
Epoch 0, Loss 0.839, LR 1.00e-05:  31%|███▏      | 31/99 [01:08&lt;02:16,  2.01s/it]
Epoch 0, Loss 0.839, LR 1.00e-05:  32%|███▏      | 32/99 [01:10&lt;02:15,  2.02s/it]
Epoch 0, Loss 1.119, LR 1.00e-05:  32%|███▏      | 32/99 [01:10&lt;02:15,  2.02s/it]
Epoch 0, Loss 1.119, LR 1.00e-05:  33%|███▎      | 33/99 [01:12&lt;02:13,  2.02s/it]
Epoch 0, Loss 0.769, LR 1.00e-05:  33%|███▎      | 33/99 [01:12&lt;02:13,  2.02s/it]
Epoch 1, Loss 0.769, LR 1.00e-05:  33%|███▎      | 33/99 [01:12&lt;02:13,  2.02s/it]
Epoch 1, Loss 0.769, LR 1.00e-05:  34%|███▍      | 34/99 [01:14&lt;02:12,  2.03s/it]
Epoch 1, Loss 0.613, LR 1.00e-05:  34%|███▍      | 34/99 [01:14&lt;02:12,  2.03s/it]
Epoch 1, Loss 0.613, LR 1.00e-05:  35%|███▌      | 35/99 [01:16&lt;02:09,  2.02s/it]
Epoch 1, Loss 0.661, LR 1.00e-05:  35%|███▌      | 35/99 [01:16&lt;02:09,  2.02s/it]
Epoch 1, Loss 0.661, LR 1.00e-05:  36%|███▋      | 36/99 [01:18&lt;02:13,  2.13s/it]
Epoch 1, Loss 0.629, LR 1.00e-05:  36%|███▋      | 36/99 [01:18&lt;02:13,  2.13s/it]
Epoch 1, Loss 0.629, LR 1.00e-05:  37%|███▋      | 37/99 [01:20&lt;02:10,  2.10s/it]
Epoch 1, Loss 0.638, LR 1.00e-05:  37%|███▋      | 37/99 [01:20&lt;02:10,  2.10s/it]
Epoch 1, Loss 0.638, LR 1.00e-05:  38%|███▊      | 38/99 [01:22&lt;02:05,  2.05s/it]
Epoch 1, Loss 0.596, LR 1.00e-05:  38%|███▊      | 38/99 [01:22&lt;02:05,  2.05s/it]
Epoch 1, Loss 0.596, LR 1.00e-05:  39%|███▉      | 39/99 [01:24&lt;02:01,  2.02s/it]
Epoch 1, Loss 0.618, LR 1.00e-05:  39%|███▉      | 39/99 [01:24&lt;02:01,  2.02s/it]
Epoch 1, Loss 0.618, LR 1.00e-05:  40%|████      | 40/99 [01:26&lt;01:58,  2.01s/it]
Epoch 1, Loss 0.539, LR 1.00e-05:  40%|████      | 40/99 [01:26&lt;01:58,  2.01s/it]
Epoch 1, Loss 0.539, LR 1.00e-05:  41%|████▏     | 41/99 [01:28&lt;01:56,  2.02s/it]
Epoch 1, Loss 0.418, LR 1.00e-05:  41%|████▏     | 41/99 [01:28&lt;01:56,  2.02s/it]
Epoch 1, Loss 0.418, LR 1.00e-05:  42%|████▏     | 42/99 [01:30&lt;01:54,  2.01s/it]
Epoch 1, Loss 0.477, LR 1.00e-05:  42%|████▏     | 42/99 [01:30&lt;01:54,  2.01s/it]
Epoch 1, Loss 0.477, LR 1.00e-05:  43%|████▎     | 43/99 [01:32&lt;01:56,  2.09s/it]
Epoch 1, Loss 0.350, LR 1.00e-05:  43%|████▎     | 43/99 [01:32&lt;01:56,  2.09s/it]
Epoch 1, Loss 0.350, LR 1.00e-05:  44%|████▍     | 44/99 [01:34&lt;01:52,  2.04s/it]
Epoch 1, Loss 0.469, LR 1.00e-05:  44%|████▍     | 44/99 [01:34&lt;01:52,  2.04s/it]
Epoch 1, Loss 0.469, LR 1.00e-05:  45%|████▌     | 45/99 [01:36&lt;01:49,  2.02s/it]
Epoch 1, Loss 0.488, LR 1.00e-05:  45%|████▌     | 45/99 [01:36&lt;01:49,  2.02s/it]
Epoch 1, Loss 0.488, LR 1.00e-05:  46%|████▋     | 46/99 [01:38&lt;01:45,  1.99s/it]
Epoch 1, Loss 0.434, LR 1.00e-05:  46%|████▋     | 46/99 [01:38&lt;01:45,  1.99s/it]
Epoch 1, Loss 0.434, LR 1.00e-05:  47%|████▋     | 47/99 [01:40&lt;01:42,  1.98s/it]
Epoch 1, Loss 0.382, LR 1.00e-05:  47%|████▋     | 47/99 [01:40&lt;01:42,  1.98s/it]
Epoch 1, Loss 0.382, LR 1.00e-05:  48%|████▊     | 48/99 [01:42&lt;01:39,  1.95s/it]
Epoch 1, Loss 0.577, LR 1.00e-05:  48%|████▊     | 48/99 [01:42&lt;01:39,  1.95s/it]
Epoch 1, Loss 0.577, LR 1.00e-05:  49%|████▉     | 49/99 [01:44&lt;01:36,  1.93s/it]
Epoch 1, Loss 0.414, LR 1.00e-05:  49%|████▉     | 49/99 [01:44&lt;01:36,  1.93s/it]
Epoch 1, Loss 0.414, LR 1.00e-05:  51%|█████     | 50/99 [01:46&lt;01:38,  2.01s/it]
Epoch 1, Loss 0.575, LR 1.00e-05:  51%|█████     | 50/99 [01:46&lt;01:38,  2.01s/it]
Epoch 1, Loss 0.575, LR 1.00e-05:  52%|█████▏    | 51/99 [01:48&lt;01:34,  1.97s/it]
Epoch 1, Loss 0.422, LR 1.00e-05:  52%|█████▏    | 51/99 [01:48&lt;01:34,  1.97s/it]
Epoch 1, Loss 0.422, LR 1.00e-05:  53%|█████▎    | 52/99 [01:50&lt;01:31,  1.94s/it]
Epoch 1, Loss 0.690, LR 1.00e-05:  53%|█████▎    | 52/99 [01:50&lt;01:31,  1.94s/it]
Epoch 1, Loss 0.690, LR 1.00e-05:  54%|█████▎    | 53/99 [01:52&lt;01:28,  1.91s/it]
Epoch 1, Loss 0.683, LR 1.00e-05:  54%|█████▎    | 53/99 [01:52&lt;01:28,  1.91s/it]
Epoch 1, Loss 0.683, LR 1.00e-05:  55%|█████▍    | 54/99 [01:54&lt;01:25,  1.91s/it]
Epoch 1, Loss 0.494, LR 1.00e-05:  55%|█████▍    | 54/99 [01:54&lt;01:25,  1.91s/it]
Epoch 1, Loss 0.494, LR 1.00e-05:  56%|█████▌    | 55/99 [01:55&lt;01:23,  1.90s/it]
Epoch 1, Loss 0.613, LR 1.00e-05:  56%|█████▌    | 55/99 [01:55&lt;01:23,  1.90s/it]
Epoch 1, Loss 0.613, LR 1.00e-05:  57%|█████▋    | 56/99 [01:57&lt;01:21,  1.90s/it]
Epoch 1, Loss 0.723, LR 1.00e-05:  57%|█████▋    | 56/99 [01:57&lt;01:21,  1.90s/it]
Epoch 1, Loss 0.723, LR 1.00e-05:  58%|█████▊    | 57/99 [01:59&lt;01:22,  1.97s/it]
Epoch 1, Loss 0.645, LR 1.00e-05:  58%|█████▊    | 57/99 [01:59&lt;01:22,  1.97s/it]
Epoch 1, Loss 0.645, LR 1.00e-05:  59%|█████▊    | 58/99 [02:01&lt;01:20,  1.96s/it]
Epoch 1, Loss 0.494, LR 1.00e-05:  59%|█████▊    | 58/99 [02:01&lt;01:20,  1.96s/it]
Epoch 1, Loss 0.494, LR 1.00e-05:  60%|█████▉    | 59/99 [02:03&lt;01:17,  1.94s/it]
Epoch 1, Loss 0.573, LR 1.00e-05:  60%|█████▉    | 59/99 [02:03&lt;01:17,  1.94s/it]
Epoch 1, Loss 0.573, LR 1.00e-05:  61%|██████    | 60/99 [02:05&lt;01:15,  1.94s/it]
Epoch 1, Loss 0.595, LR 1.00e-05:  61%|██████    | 60/99 [02:05&lt;01:15,  1.94s/it]
Epoch 1, Loss 0.595, LR 1.00e-05:  62%|██████▏   | 61/99 [02:07&lt;01:13,  1.94s/it]
Epoch 1, Loss 0.381, LR 1.00e-05:  62%|██████▏   | 61/99 [02:07&lt;01:13,  1.94s/it]
Epoch 1, Loss 0.381, LR 1.00e-05:  63%|██████▎   | 62/99 [02:09&lt;01:11,  1.93s/it]
Epoch 1, Loss 0.641, LR 1.00e-05:  63%|██████▎   | 62/99 [02:09&lt;01:11,  1.93s/it]
Epoch 1, Loss 0.641, LR 1.00e-05:  64%|██████▎   | 63/99 [02:11&lt;01:10,  1.95s/it]
Epoch 1, Loss 0.548, LR 1.00e-05:  64%|██████▎   | 63/99 [02:11&lt;01:10,  1.95s/it]
Epoch 1, Loss 0.548, LR 1.00e-05:  65%|██████▍   | 64/99 [02:13&lt;01:11,  2.04s/it]
Epoch 1, Loss 0.494, LR 1.00e-05:  65%|██████▍   | 64/99 [02:13&lt;01:11,  2.04s/it]
Epoch 1, Loss 0.494, LR 1.00e-05:  66%|██████▌   | 65/99 [02:15&lt;01:07,  1.98s/it]
Epoch 1, Loss 0.712, LR 1.00e-05:  66%|██████▌   | 65/99 [02:15&lt;01:07,  1.98s/it]
Epoch 1, Loss 0.712, LR 1.00e-05:  67%|██████▋   | 66/99 [02:17&lt;01:04,  1.95s/it]
Epoch 1, Loss 0.335, LR 1.00e-05:  67%|██████▋   | 66/99 [02:17&lt;01:04,  1.95s/it]
Epoch 2, Loss 0.335, LR 1.00e-05:  67%|██████▋   | 66/99 [02:17&lt;01:04,  1.95s/it]
Epoch 2, Loss 0.335, LR 1.00e-05:  68%|██████▊   | 67/99 [02:19&lt;01:01,  1.93s/it]
Epoch 2, Loss 0.411, LR 1.00e-05:  68%|██████▊   | 67/99 [02:19&lt;01:01,  1.93s/it]
Epoch 2, Loss 0.411, LR 1.00e-05:  69%|██████▊   | 68/99 [02:21&lt;01:00,  1.96s/it]
Epoch 2, Loss 0.455, LR 1.00e-05:  69%|██████▊   | 68/99 [02:21&lt;01:00,  1.96s/it]
Epoch 2, Loss 0.455, LR 1.00e-05:  70%|██████▉   | 69/99 [02:23&lt;00:57,  1.93s/it]
Epoch 2, Loss 0.369, LR 1.00e-05:  70%|██████▉   | 69/99 [02:23&lt;00:57,  1.93s/it]
Epoch 2, Loss 0.369, LR 1.00e-05:  71%|███████   | 70/99 [02:25&lt;00:55,  1.91s/it]
Epoch 2, Loss 0.384, LR 1.00e-05:  71%|███████   | 70/99 [02:25&lt;00:55,  1.91s/it]
Epoch 2, Loss 0.384, LR 1.00e-05:  72%|███████▏  | 71/99 [02:27&lt;00:55,  1.99s/it]
Epoch 2, Loss 0.370, LR 1.00e-05:  72%|███████▏  | 71/99 [02:27&lt;00:55,  1.99s/it]
Epoch 2, Loss 0.370, LR 1.00e-05:  73%|███████▎  | 72/99 [02:29&lt;00:53,  1.97s/it]
Epoch 2, Loss 0.396, LR 1.00e-05:  73%|███████▎  | 72/99 [02:29&lt;00:53,  1.97s/it]
Epoch 2, Loss 0.396, LR 1.00e-05:  74%|███████▎  | 73/99 [02:31&lt;00:50,  1.94s/it]
Epoch 2, Loss 0.337, LR 1.00e-05:  74%|███████▎  | 73/99 [02:31&lt;00:50,  1.94s/it]
Epoch 2, Loss 0.337, LR 1.00e-05:  75%|███████▍  | 74/99 [02:33&lt;00:48,  1.92s/it]
Epoch 2, Loss 0.206, LR 1.00e-05:  75%|███████▍  | 74/99 [02:33&lt;00:48,  1.92s/it]
Epoch 2, Loss 0.206, LR 1.00e-05:  76%|███████▌  | 75/99 [02:34&lt;00:45,  1.91s/it]
Epoch 2, Loss 0.247, LR 1.00e-05:  76%|███████▌  | 75/99 [02:34&lt;00:45,  1.91s/it]
Epoch 2, Loss 0.247, LR 1.00e-05:  77%|███████▋  | 76/99 [02:36&lt;00:43,  1.91s/it]
Epoch 2, Loss 0.183, LR 1.00e-05:  77%|███████▋  | 76/99 [02:36&lt;00:43,  1.91s/it]
Epoch 2, Loss 0.183, LR 1.00e-05:  78%|███████▊  | 77/99 [02:38&lt;00:41,  1.90s/it]
Epoch 2, Loss 0.236, LR 1.00e-05:  78%|███████▊  | 77/99 [02:38&lt;00:41,  1.90s/it]
Epoch 2, Loss 0.236, LR 1.00e-05:  79%|███████▉  | 78/99 [02:40&lt;00:42,  2.01s/it]
Epoch 2, Loss 0.220, LR 1.00e-05:  79%|███████▉  | 78/99 [02:40&lt;00:42,  2.01s/it]
Epoch 2, Loss 0.220, LR 1.00e-05:  80%|███████▉  | 79/99 [02:42&lt;00:40,  2.01s/it]
Epoch 2, Loss 0.186, LR 1.00e-05:  80%|███████▉  | 79/99 [02:42&lt;00:40,  2.01s/it]
Epoch 2, Loss 0.186, LR 1.00e-05:  81%|████████  | 80/99 [02:44&lt;00:37,  1.99s/it]
Epoch 2, Loss 0.251, LR 1.00e-05:  81%|████████  | 80/99 [02:44&lt;00:37,  1.99s/it]
Epoch 2, Loss 0.251, LR 1.00e-05:  82%|████████▏ | 81/99 [02:46&lt;00:35,  1.95s/it]
Epoch 2, Loss 0.315, LR 1.00e-05:  82%|████████▏ | 81/99 [02:46&lt;00:35,  1.95s/it]
Epoch 2, Loss 0.315, LR 1.00e-05:  83%|████████▎ | 82/99 [02:48&lt;00:33,  1.94s/it]
Epoch 2, Loss 0.147, LR 1.00e-05:  83%|████████▎ | 82/99 [02:48&lt;00:33,  1.94s/it]
Epoch 2, Loss 0.147, LR 1.00e-05:  84%|████████▍ | 83/99 [02:50&lt;00:31,  1.94s/it]
Epoch 2, Loss 0.208, LR 1.00e-05:  84%|████████▍ | 83/99 [02:50&lt;00:31,  1.94s/it]
Epoch 2, Loss 0.208, LR 1.00e-05:  85%|████████▍ | 84/99 [02:52&lt;00:29,  1.94s/it]
Epoch 2, Loss 0.187, LR 1.00e-05:  85%|████████▍ | 84/99 [02:52&lt;00:29,  1.94s/it]
Epoch 2, Loss 0.187, LR 1.00e-05:  86%|████████▌ | 85/99 [02:54&lt;00:28,  2.03s/it]
Epoch 2, Loss 0.394, LR 1.00e-05:  86%|████████▌ | 85/99 [02:54&lt;00:28,  2.03s/it]
Epoch 2, Loss 0.394, LR 1.00e-05:  87%|████████▋ | 86/99 [02:56&lt;00:25,  1.99s/it]
Epoch 2, Loss 0.326, LR 1.00e-05:  87%|████████▋ | 86/99 [02:56&lt;00:25,  1.99s/it]
Epoch 2, Loss 0.326, LR 1.00e-05:  88%|████████▊ | 87/99 [02:58&lt;00:23,  1.97s/it]
Epoch 2, Loss 0.227, LR 1.00e-05:  88%|████████▊ | 87/99 [02:58&lt;00:23,  1.97s/it]
Epoch 2, Loss 0.227, LR 1.00e-05:  89%|████████▉ | 88/99 [03:00&lt;00:21,  1.96s/it]
Epoch 2, Loss 0.256, LR 1.00e-05:  89%|████████▉ | 88/99 [03:00&lt;00:21,  1.96s/it]
Epoch 2, Loss 0.256, LR 1.00e-05:  90%|████████▉ | 89/99 [03:02&lt;00:19,  1.96s/it]
Epoch 2, Loss 0.334, LR 1.00e-05:  90%|████████▉ | 89/99 [03:02&lt;00:19,  1.96s/it]
Epoch 2, Loss 0.334, LR 1.00e-05:  91%|█████████ | 90/99 [03:04&lt;00:17,  1.94s/it]
Epoch 2, Loss 0.330, LR 1.00e-05:  91%|█████████ | 90/99 [03:04&lt;00:17,  1.94s/it]
Epoch 2, Loss 0.330, LR 1.00e-05:  92%|█████████▏| 91/99 [03:06&lt;00:15,  1.94s/it]
Epoch 2, Loss 0.259, LR 1.00e-05:  92%|█████████▏| 91/99 [03:06&lt;00:15,  1.94s/it]
Epoch 2, Loss 0.259, LR 1.00e-05:  93%|█████████▎| 92/99 [03:08&lt;00:14,  2.02s/it]
Epoch 2, Loss 0.283, LR 1.00e-05:  93%|█████████▎| 92/99 [03:08&lt;00:14,  2.02s/it]
Epoch 2, Loss 0.283, LR 1.00e-05:  94%|█████████▍| 93/99 [03:11&lt;00:13,  2.17s/it]
Epoch 2, Loss 0.271, LR 1.00e-05:  94%|█████████▍| 93/99 [03:11&lt;00:13,  2.17s/it]
Epoch 2, Loss 0.271, LR 1.00e-05:  95%|█████████▍| 94/99 [03:21&lt;00:23,  4.62s/it]
Epoch 2, Loss 0.180, LR 1.00e-05:  95%|█████████▍| 94/99 [03:21&lt;00:23,  4.62s/it]
Epoch 2, Loss 0.180, LR 1.00e-05:  96%|█████████▌| 95/99 [03:23&lt;00:15,  3.83s/it]
Epoch 2, Loss 0.325, LR 1.00e-05:  96%|█████████▌| 95/99 [03:23&lt;00:15,  3.83s/it]
Epoch 2, Loss 0.325, LR 1.00e-05:  97%|█████████▋| 96/99 [03:25&lt;00:09,  3.27s/it]
Epoch 2, Loss 0.280, LR 1.00e-05:  97%|█████████▋| 96/99 [03:25&lt;00:09,  3.27s/it]
Epoch 2, Loss 0.280, LR 1.00e-05:  98%|█████████▊| 97/99 [03:27&lt;00:05,  2.90s/it]
Epoch 2, Loss 0.228, LR 1.00e-05:  98%|█████████▊| 97/99 [03:27&lt;00:05,  2.90s/it]
Epoch 2, Loss 0.228, LR 1.00e-05:  99%|█████████▉| 98/99 [03:29&lt;00:02,  2.65s/it]
Epoch 2, Loss 0.380, LR 1.00e-05:  99%|█████████▉| 98/99 [03:29&lt;00:02,  2.65s/it]
Epoch 2, Loss 0.380, LR 1.00e-05: 100%|██████████| 99/99 [03:31&lt;00:00,  2.56s/it]
Epoch 2, Loss 0.106, LR 1.00e-05: 100%|██████████| 99/99 [03:31&lt;00:00,  2.56s/it]/usr/local/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/usr/local/lib/python3.11/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:680: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
Using BNB DORA 1

Epoch 2, Loss 0.106, LR 1.00e-05: 100%|██████████| 99/99 [03:33&lt;00:00,  2.16s/it]
Finished training 0
CUDA event elapsed time: 211.839921875 sec
time_taken: 211.839921875
Rank 0: Before forward: 1.62 GiB
Rank 0: After forward: 2.46 GiB
Rank 0: After backward: 2.64 GiB
Rank 0: Peak allocated memory: 1.41 GiB
Rank 0: Peak reserved memory:  2.65 GiB
Saving trained LoRA weights.
Done 0
Training completed: 0
🎉 Model saved successfully!
📁 Saved files:
  📄 model_state_dict.safetensors</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> zipfile</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> download_model():</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Package the model for download"""</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.path.exists(<span class="st">"uganda_clinical_model"</span>):</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"📦 Packaging model for download..."</span>)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create a zip file</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> zipfile.ZipFile(<span class="st">"uganda_clinical_qdora_model.zip"</span>, <span class="st">"w"</span>, zipfile.ZIP_DEFLATED) <span class="im">as</span> zipf:</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> root, dirs, files <span class="kw">in</span> os.walk(<span class="st">"uganda_clinical_model"</span>):</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> files:</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>                    file_path <span class="op">=</span> os.path.join(root, <span class="bu">file</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>                    zipf.write(file_path, file_path)</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"✅ Model packaged as uganda_clinical_qdora_model.zip"</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"📊 File size: </span><span class="sc">{</span>os<span class="sc">.</span>path<span class="sc">.</span>getsize(<span class="st">'uganda_clinical_qdora_model.zip'</span>) <span class="op">/</span> <span class="dv">1024</span> <span class="op">/</span> <span class="dv">1024</span><span class="sc">:.1f}</span><span class="ss"> MB"</span>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In Jupyter, this will be available for download</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"💾 You can download this file from the Jupyter file browser"</span>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"❌ No model directory found. Run training with --save_model true first"</span>)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>download_model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>📦 Packaging model for download...
✅ Model packaged as uganda_clinical_qdora_model.zip
📊 File size: 217.9 MB
💾 You can download this file from the Jupyter file browser</code></pre>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_model():</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Test the trained model"""</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the model for inference</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModelForCausalLM</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> torch</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    model_path <span class="op">=</span> <span class="st">"./uganda_clinical_model"</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.exists(model_path):</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"❌ Model not found. Train with --save_model true first"</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"🔄 Loading trained model..."</span>)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load tokenizer and model</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"meta-llama/Llama-2-7b-hf"</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>            model_path,</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>            torch_dtype<span class="op">=</span>torch.float16,</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>            device_map<span class="op">=</span><span class="st">"auto"</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"✅ Model loaded successfully!"</span>)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Test with a medical question</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>        test_prompt <span class="op">=</span> <span class="st">"""Below is an instruction that describes a task. Write a response that appropriately completes the request.</span></span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a><span class="st">### Instruction:</span></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a><span class="st">I have fever, general body weakness, joint paints and have been getting by mosquitoes often. what could be the cause ?</span></span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a><span class="st">### Response:"""</span></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">🧪 Testing model..."</span>)</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"Question: What are the symptoms of malaria?"</span>)</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Model Response:"</span>)</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generate response</span></span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> tokenizer(test_prompt, return_tensors<span class="op">=</span><span class="st">"pt"</span>)</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model.generate(</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>                inputs.input_ids,</span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>                max_length<span class="op">=</span>inputs.input_ids.shape[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">150</span>,</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>                temperature<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>                do_sample<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>                pad_token_id<span class="op">=</span>tokenizer.eos_token_id</span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>        response <span class="op">=</span> tokenizer.decode(outputs[<span class="dv">0</span>], skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Extract just the response part</span></span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>        response_only <span class="op">=</span> response.split(<span class="st">"### Response:"</span>)[<span class="op">-</span><span class="dv">1</span>].strip()</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(response_only)</span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"❌ Error loading model: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-59"><a href="#cb29-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Run after you've saved the model</span></span>
<span id="cb29-60"><a href="#cb29-60" aria-hidden="true" tabindex="-1"></a>test_model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>🔄 Loading trained model...
❌ Error loading model: Unrecognized model in ./uganda_clinical_model. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth</code></pre>
</div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inspect_saved_model():</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Check what files were actually saved"""</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    model_dir <span class="op">=</span> <span class="st">"./uganda_clinical_model"</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> os.path.exists(model_dir):</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"📁 Files in uganda_clinical_model:"</span>)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> os.listdir(model_dir):</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>            file_path <span class="op">=</span> os.path.join(model_dir, <span class="bu">file</span>)</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>            size <span class="op">=</span> os.path.getsize(file_path) <span class="op">/</span> <span class="dv">1024</span> <span class="op">/</span> <span class="dv">1024</span>  <span class="co"># MB</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  📄 </span><span class="sc">{</span><span class="bu">file</span><span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>size<span class="sc">:.1f}</span><span class="ss"> MB)"</span>)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check for specific files</span></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        expected_files <span class="op">=</span> [<span class="st">"adapter_config.json"</span>, <span class="st">"adapter_model.bin"</span>, <span class="st">"adapter_model.safetensors"</span>]</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> expected <span class="kw">in</span> expected_files:</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> os.path.exists(os.path.join(model_dir, expected)):</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"✅ Found: </span><span class="sc">{</span>expected<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"❌ Missing: </span><span class="sc">{</span>expected<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"❌ Model directory not found"</span>)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>inspect_saved_model()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>📁 Files in uganda_clinical_model:
  📄 model_state_dict.safetensors (275.4 MB)
❌ Missing: adapter_config.json
❌ Missing: adapter_model.bin
❌ Missing: adapter_model.safetensors</code></pre>
</div>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>