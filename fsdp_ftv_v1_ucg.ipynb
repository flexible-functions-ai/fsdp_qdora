{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4776e711-2e91-437a-b6df-494e7d65eaae",
   "metadata": {},
   "source": [
    "# Efficient finetuning of Llama 3 with FSDP QDora on the Uganda Clinical Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc675734-bd90-4d01-803d-4933d93086f5",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The Ugandan Ministry of Health and its partners published the Uganda Clinical Guidelines to help give practitioners access to the latest up-to-date information on how to diagnose and manage common health conditions in Uganda.\n",
    "\n",
    "You can find a link to the Uganda clinical guidelines [here](https://library.health.go.ug/uganda-clinical-guidelines-2023).\n",
    "\n",
    "To quote the Clinical guidelines book itself,\n",
    "\n",
    "> **What is the aim of the UCG?**\n",
    "> \n",
    ">The UCG aims to provide summarized easy-to-use, practical, complete and useful information on how to quickly and correctly diagnose and manage common conditions you are likely to encounter. This will ensure that patients receive the best possible clinical services and obtain prompt and effective relief from or cure of their complaint, thereby making the most appropriate use of scarce diagnostic and clinical resources, including medicines. It should, however, be emphasised that the UCG does not replace or substituteavailable textbooks on the subject.\n",
    "\n",
    "> **Why is the UCG necessary?**\n",
    ">\n",
    "> Medicine is an ever-evolving and expanding field in terms of needs and knowledge. The UCG helps the country to prioritize and effectively use limited resources by guiding the procurement system to ensure the availability of the most needed medicines and supplies. In the context of new knowledge and changing priorities, as a tool, the UCG assists health workers in their daily practice by providing information in an easy-to-follow and practical format.\n",
    "\n",
    "With this, we are experimenting with fine-tuning a large language model like llama on these guidelines. The hope is that this model can be used as a basis for an assistive tool.\n",
    "\n",
    "The Uganda clinical guidelines have over 1000+ pages containing information such as clinical features, causes, differential diagnoses, treatment, and prevention options for many common health complaints in Uganda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9150cc18-7fc3-4873-8989-93966e1455db",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "I shall be fine-tuning a `Llama3`70B parameter model using FSDP QDora, an open-source system first introduced in this answer.ai [post](https://www.answer.ai/posts/2024-04-26-fsdp-qdora-llama3.html). This system is an extension of FSDP QLora which is a combination of FSDP and QLora.\n",
    "\n",
    "### Background\n",
    "\n",
    "QLora was made possible by 2 advances in neural networks, namely quantization and LORA. Quantization reduces the number of bits used to represent parameters in a model, here we find ourselves trading off between zero shot accuracy at inference time and model bits.  \n",
    "\n",
    "For example instead of using 32 or 16 bits to store the weights of a neural network, we can use a smaller number of bits, like 4. A 4-bit number is equivalent to (2 x 2 x 2 x 2) and has only 16 possible values. \n",
    "\n",
    "*Tim Dettmers & Luke Zettlemoyer* released a paper that showed that they ran an experiments to determine the bit precision that maximizes one shot learning. We can see that the Zero-shot accuracy increases steadily for fixed model bits from 16 up to 4 bit quantitzation precision, When we reach 3 bits, we can see that the relationship reverses. Refer to the image below from the original [The case for 4-bit precision:\n",
    "k-bit Inference Scaling Laws](https://arxiv.org/pdf/2212.09720) paper.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc360a7d",
   "metadata": {},
   "source": [
    "\n",
    " 4 bit precision was shown to be the best precision value that maximises total model bits and zero shot accuracy. zero shot accuracy and the Total model bits \n",
    "\n",
    "\n",
    "It is possible to create these 4-bit models using the **bitsandbytes** library by Tim Dettmers. The downside to this \n",
    "\n",
    "This method enables us to efficiently and cheaply fine-tune large models with billions of parameters on consumer hardware like standard gaming GPUS's, as opposed to using data center-class GPUs, which cost thousands of dollars more. Remember, each parameter usually takes 16 bits (2 bytes), so thatâ€™s 70*2=140GB to even store the weights without thinking of the other data like the gradients etc.(Full Sharded Data Parallel)[https://engineering.fb.com/2021/07/15/open-source/fsdp/] developed by Meta, which helps us distribute our parameters across multiple GPUs, allowing us to use them in parallel. This used in conjuction with QDora which brings quantitization to \n",
    "\n",
    "Quantization is a technique where instead of using 32 or 16 bits to store the weights of a neural network, 4 are used. It is possible to create these 4 bit models using the **bitsandbytes** library by Tim Dettmers. The downside to this "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
